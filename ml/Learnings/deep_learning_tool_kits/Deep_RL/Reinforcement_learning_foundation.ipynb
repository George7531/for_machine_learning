{"cells":[{"cell_type":"markdown","metadata":{"id":"2FOWEJRjLofL"},"source":["# day 343"]},{"cell_type":"markdown","metadata":{"id":"eGEgVNruLuxu"},"source":["# what is Reinforcement learning?\n","* it is about automating the control(decision making) tasks.\n","* for example when you first learn to play a video game, you move forward, fight enemies and get various rewards and if you had made a bad decision then you may die or lose your weapon or gold coins that you have gone through a lot of paint to collect and so on. from this you may know what to do in the game and what not, Agents you would deploy via Reinforcement learning would do something similar as well, they would learn about their environment and then make effective decisions to get more rewards and less errors. Errors are sins.\n","\n","## formal definition:\n","* Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n","\n","\n","## RL: a process of state,action, reward and repeat the loop:\n","![sfsfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg)\n","\n","\n","## An agent trying to play a game:\n","![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg)\n","\n","* Our Agent receives state s0,from the Environment — we receive the first frame of our game (Environment).\n","* Based on that state s0, the Agent takes action a0,our Agent will move to the right.\n","* The environment goes to a new state s1,— new frame.\n","The environment gives some reward r1, to the Agent — we’re not dead (Positive Reward +1).\n","* This RL loop outputs a sequence of state, action, reward and next state.\n","\n","![sdfsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg)\n","\n","* The agent's goal is to maximize its cumulative reward, called the expected return.\n","\n","\n","## The reward Hypothesis: the central ideal of reinforcement learning:\n","* why Agents' goal is to maximize the reward(expected return)?\n","* This reward hypothesis states that: All goals are pointing to only one thing that is to maximize the expected return from action(s).\n","* That is why in RL, `the best behaviour of the AI agent is mapped with them maximizing the expected return of our goal(s)`.\n","* `Description of our goals -> AI agents -> actions -> maximize our expected return of our goals.`\n","\n","## Markov Process:\n","* In a simplified tone it Markov Decision process implies that AI agents only need to know their current states to decide what actions to take next, they need not have the memory of all the states they were in before and they need not have the memory all the actions they took before.\n","\n","## what are states?\n","* states/observations are the information our agents get from their environment.\n","* in the case of a video game, it is the screenshot of how the game looks currently.\n","* in the case of stock in the share market, it is the current price of that stock.\n","\n","\n","## difference between states and observations:\n","* state is fully observed environment and an observation is a partially observed environment.\n","* graphical explanations are given below.\n","\n","![fwsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg)\n","In chess game, we receive a state from the environment since we have access to the whole check board information.\n","\n","![sfsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg)\n","In Super Mario Bros, we only see the part of the level close to the player, so we receive an observation.\n","\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg)\n","\n","\n","# Action space:\n","* Action space is a set of all actions an agent can perform in an environment\n","* it can be discrete(finite) or continous(infinite).\n","* for example in the game of mario there are only 4 actions the agent has to perform to navigate itself in the world of mario (left,right,up and crouch), but a self driving car may turn 20.1 degree left or 20.23424 degree right and honk the horn therefore the number of actions a self driving car can perform is infinite.\n","\n","![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg)\n","in Super Mario Bros, we have only 4 possible actions: left, right, up (jumping) and down (crouching).\n","\n","![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/self_driving_car.jpg)\n","A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21,1°, 21,2°, honk, turn right 20°…\n","\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg)\n","\n","## Rewards and discounting:\n","* Rewards are rewards the agent would receive on successful completion of the task\n","* discounts are predictors of penalties.\n","* Thanks to Rewards our agents will know whether their actions are good or bad.\n","\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg)\n","![sds](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg)\n","\n","* However, in reality, we can’t just add them like that. The rewards that come sooner (at the beginning of the game) are more likely to happen since they are more predictable than the long-term future reward.\n","\n","* Let’s say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (that can move too). The mouse’s goal is to eat the maximum amount of cheese before being eaten by the cat.\n","\n","![fs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg)\n","\n","As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).\n","\n","Consequently, the reward near the cat, even if it is bigger (more cheese), will be more discounted since we’re not really sure we’ll be able to eat it.\n","\n","To discount the rewards, we proceed like this:\n","\n","1. We define a discount rate called gamma. It must be between 0 and 1.Most of the time between 0.95 and 0.99.\n","The larger the gamma, the smaller the discount. This means our agent cares more about the long-term reward.\n","On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n","\n","2. Then, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.\n","\n","Our discounted expected cumulative reward is:\n","![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3MA0_uZaDv3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bARhC1x9Lu77"},"source":["# Types of tasks:\n","There are 2 types of tasks which are\n","1. Episodic.\n","2. continuing.\n","\n","# Episodic tasks:\n","* Tasks with an end (terminal state) are called Episodic tasks i.e the game of mario, God of war 5 ragnorok, writing emails to 100 people and etc..\n","* Tasks without an end (terminal state) are called continuous tasks i.e stock investing, real estate investing, and infinite loops.\n","\n","![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/tasks.jpg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_xgtQUBLvDd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"WZnCvt11LvGO"},"source":["# exploration and exploitation tradeoff:\n","\n","* exploration: is exploring the environment to see if there is a better opportunity.\n","* exploition: is using the current information you have about the environment and take as much as resources as possible from the environment with that information.\n","\n","## examples:\n","![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_1.jpg)\n","\n","* In this game, our mouse can have an infinite amount of small cheese (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).\n","\n","* However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the nearest source of rewards, even if this source is small (exploitation).\n","\n","* But if our agent does a little bit of exploration, it can discover the big reward (the pile of big cheese).\n","\n","* This is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment.\n","\n","* Therefore, we must define a rule that helps to handle this trade-off. We’ll see the different ways to handle it in the future units.\n","\n","\n","![sdfssdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_2.jpg)\n","\n","* **Exploitation**: You go to the same one that you know is good every day and take the risk to miss another better restaurant.\n","\n","\n","* **Exploration**: Try restaurants you never went to before, with the risk of having a bad experience but the probable opportunity of a fantastic experience.\n","\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfZuCIghLvI3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"SJTyQLMEzGgo"},"source":["# Two main approaches for solving RL problems:\n","\n","1. policy based methods.\n","2. value based methods.\n","\n","# policy based methods:\n","\n","* The Policy π is the brain of our Agent, it’s the function that tells us what action to take given the state we are in. So it defines the agent’s behavior at a given time.\n","\n","![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_1.jpg)\n","\n","Think of policy as the brain of our agent, the function that will tell us the action to take given a state.\n","\n","\n","1. Deterministic approach:\n","* each state is mapped to a particular action in deterministic approach\n","* so each time an agent reaches a particular observes a particular state in its environment it will always get to do the same pre determined set of action(s).\n","\n","2. probablistic distribution approach:\n","* the set of actions the agent can perform is mapped with some probability of being executed corresponding to each state in the environment.\n","* so, the agent may choose the action with the highest probability corresponding to the state it is in.\n","\n","\n","![fs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_2.jpg)\n","As we can see here, the policy (deterministic) directly indicates the action to take for each step.\n","\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_3.jpg)\n","\n","action = policy_fn(state).\n","\n","![fss](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_4.jpg)\n","\n","Stochastic: outputs a probability distribution over actions.\n","\n","![sdfsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_5.jpg)\n","\n","olicy(actions | state) = probability distribution over the set of actions given the current state\n","\n","![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy-based.png)\n","Given an initial state, our stochastic policy will output probability distributions over the possible actions at that state.\n","\n","## Recap:\n","![fsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg)\n","![fsdfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg)\n","\n","# value based methods:\n","* it calculates the value of being at each state in the environment and optimizes to choose the best state that will yield the maximum cumulative reward.\n","* the value of a state is calculated by computing the expected discount rate of being at a state given a state.\n","\n","![sdfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg)\n","\n","Here we see that our value function defined values for each possible state.\n","\n","![sdff](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_2.jpg)\n","\n","Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.\n","\n","## Recap:\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg)\n","![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wgzfzxBBweI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"R4XDlZMQBwzP"},"source":["# \"DEEp\" in Deep Reinforcement Learning:\n","\n","* 'DEEp' in deep Reinforcement learning is because we would use neural networks to solve the reinforcement learning problems which was once a field of its own now as we have made some great strides in optimizing deep learning algorithms and see some fantastic applications in the likeness of transformers, attention algorithms and so on, it's time to merge these both worlds to see great many challenges being resolved.\n","\n","![sfsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8MbdJA7JCcv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NGoLAZyLLd3A"},"source":["# Time to code to get the following result;\n","\n","we’re going to train our agent, a Lunar Lander, to land correctly on the moon. To do that, the agent needs to learn to adapt its speed and position (horizontal, vertical, and angular) to land correctly"]},{"cell_type":"markdown","metadata":{"id":"aAAhSbx6DHt2"},"source":["![fsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/lunarLander.gif)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrXRJ-DlBw4o"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_JybAxAcBw7v"},"source":["# Installing essentials"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6880,"status":"ok","timestamp":1712847897515,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"l32zCo4bOG60","outputId":"cc51fa2c-a3a8-4750-fdb3-d177a0d5dc1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n","Suggested packages:\n","  swig-doc swig-examples swig4.0-examples swig4.0-doc\n","The following NEW packages will be installed:\n","  swig swig4.0\n","0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 1,116 kB of archives.\n","After this operation, 5,542 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n","Fetched 1,116 kB in 1s (1,384 kB/s)\n","Selecting previously unselected package swig4.0.\n","(Reading database ... 121752 files and directories currently installed.)\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["!apt install swig cmake"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117248,"status":"ok","timestamp":1712848014760,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"GT8W948qLna2","outputId":"c389ba30-b488-4b25-c62d-3316b1f44d7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting stable-baselines3==2.0.0a5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading stable_baselines3-2.0.0a5-py3-none-any.whl (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting swig (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n","  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface_sb3 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4))\n","  Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n","Collecting gymnasium==0.28.1 (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.25.2)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.2.1+cu121)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.0.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.7.1)\n","Collecting jax-jumpy>=1.0.0 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.10.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","INFO: pip is looking at multiple versions of gymnasium[box2d] to determine which version is compatible with other requirements. This could take a while.\n","Collecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n","  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting box2d-py==2.3.5 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pygame==2.1.3 (from gymnasium==0.28.1->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (0.20.3)\n","Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (6.0.1)\n","Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (1.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.13.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (4.66.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.50.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface_sb3->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0a5->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.3.0)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349113 sha256=ce75d356d17e1110079d3ba6aeaf3daa0f0174da3398a4c1294692bb8c1e6a33\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: swig, farama-notifications, box2d-py, pygame, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jax-jumpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gymnasium, nvidia-cusolver-cu12, huggingface_sb3, stable-baselines3\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 huggingface_sb3-3.0 jax-jumpy-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pygame-2.1.3 stable-baselines3-2.0.0a5 swig-4.2.1\n"]}],"source":["!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24703,"status":"ok","timestamp":1712848039457,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"Rsi6Uma4Lneu","outputId":"9b034722-d7e4-4606-e881-062b8ae32cb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [1 I\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Wai\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","\r                                                                    \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","\r0% [Waiting for headers] [Waiting for headers]\r                                              \rGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","\r0% [Waiting for headers] [5 InRelease 14.2 kB/110 kB 13%] [Waiting for headers]\r                                                                               \rHit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","\r                                                                               \r0% [Waiting for headers] [5 InRelease 14.2 kB/110 kB 13%]\r                                                         \rHit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\r0% [Waiting for headers] [5 InRelease 14.2 kB/110 kB 13%] [Waiting for headers]\r                                                                               \rHit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,691 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,974 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,174 kB]\n","Fetched 8,510 kB in 2s (3,580 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","\u001b[1;31mE: \u001b[0mUnable to locate package python-opengl\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 50 not upgraded.\n","Need to get 7,813 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n","Fetched 7,813 kB in 1s (7,548 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 122505 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n"]}],"source":["\"\"\"\n","During the notebook, we’ll need to generate a replay video. To do so, with colab,\n","we need to have a virtual screen to be able to render the environment (and thus record the frames).\n","\"\"\"\n","\n","! sudo apt-get update\n","! apt install python-opengl\n","! apt install ffmpeg\n","! apt install xvfb\n","! pip3 install pyvirtualdisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AE9hKD-uMiPm"},"outputs":[],"source":["# to kill the current session so that all the newly installed packages will start showing up results when called for.\n","import os\n","\n","os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_yvXGiXPZHN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1712848047316,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"Vc0HdINDPZPz","outputId":"7c68a1eb-50c2-40e6-ee86-c52817bc4d40"},"outputs":[{"data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x79e68b796020>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaL-ectXPZTS"},"outputs":[],"source":["import gymnasium # we'll use gymnasium a lot in RL\n","from huggingface_sb3 import load_from_hub, package_to_hub\n","from huggingface_hub import (\n","    notebook_login,\n",") # to be able to upload models to the hub\n","\n","# Stable_baseline3 is a set of reliable implementations of reinforcement learning algorithms in PyTorch.\n","from stable_baselines3 import PPO # Proximal Policy Optimization (is an algorithm that combines both value based RL and policy based RL)\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zl7E4rUoLnhe"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"pfQKRMqVLnkJ"},"source":["![sdff](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg)\n","\n","## overview:\n","* the environment gives the state\n","* state of the environment is fed to the agent.\n","* agent takes an apt action.\n","* environment rewards or punishes the agent.\n","* the state is updated.\n","* the agent makes an improved action.\n","\n","\n","## how gymnasium works?\n","With Gymnasium:\n","\n","1️⃣ We create our environment using gymnasium.make()\n","\n","2️⃣ We reset the environment to its initial state with observation = env.reset()\n","\n","At each step:\n","\n","3️⃣ Get an action using our model (in our example we take a random action)\n","\n","4️⃣ Using env.step(action), we perform this action in the environment and get\n","\n","observation: The new state (st+1)\n","reward: The reward we get after executing the action\n","terminated: Indicates if the episode terminated (agent reach the terminal state)\n","truncated: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\n","info: A dictionary that provides additional information (depends on the environment).\n","For more explanations check this 👉 https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n","\n","* If the episode is terminated:\n","\n","We reset the environment to its initial state with observation = env.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NF0rSY7WOve"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZJP7kw6Xm9E"},"outputs":[],"source":["import gymnasium as gym\n","\n","# lets make the environment\n","env = gym.make('LunarLander-v2')\n","\n","# lets reset the reset environment\n","observation,info = env.reset()\n","\n","# make the agent take actions\n","for _ in range(20):\n","  action = env.action_space.sample() # taking random actions and improvising.\n","\n","  # pass the action to the next step of the environment and observe the recordings\n","  observation,reward,terminated,truncated,info = env.step(action)\n","\n","  # if terminated or truncated(went out of bounds or timelapsed) reset the env and update the observation and info\n","  if terminated or truncated:\n","    print('Environment is reset')\n","    observation,info = env.reset()\n","\n","# close the environment after the task is complete\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Orp30YGXnAO"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GvsCC4Dm12HP"},"source":["# Create the LunarLander environment 🌛 and understand how it works:\n","\n","## observation space:\n","the observation is a vector of size 8, where each value contains different information about the lander:\n","\n","* Horizontal pad coordinate (x)\n","* Vertical pad coordinate (y)\n","* Horizontal speed (x)\n","* Vertical speed (y)\n","* Angle\n","* Angular speed\n","* If the left leg contact point has touched the land (boolean)\n","* If the right leg contact point has touched the land (boolean)\n","\n","## action space:\n","The action space (the set of possible actions the agent can take) is discrete with 4 actions available 🎮:\n","\n","* Action 0: Do nothing,\n","* Action 1: Fire left orientation engine,\n","* Action 2: Fire the main engine,\n","* Action 3: Fire right orientation engine.\n","\n","# conditions for reward:\n","For each step, the reward:\n","\n","* Is increased/decreased the closer/further the lander is to the landing pad.\n","* Is increased/decreased the slower/faster the lander is moving.\n","* Is decreased the more the lander is tilted (angle not horizontal).\n","* Is increased by 10 points for each leg that is in contact with the ground.\n","* Is decreased by 0.03 points each frame a side engine is firing.\n","* Is decreased by 0.3 points each frame the main engine is firing.\n","* The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n","* An episode is considered a solution if it scores at least 200 points.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79xqlIU2XbNW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1712848656477,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"c0odCs2H12KR","outputId":"921fe1b8-e8c0-4a3f-899c-bb2e591a579c"},"outputs":[{"name":"stdout","output_type":"stream","text":["observation space shape: (8,)\n","A sample observation space: [ 1.08265505e+01  8.43441162e+01 -4.47417164e+00 -1.32017839e+00\n"," -2.65790009e+00  6.04623146e-02  3.18004936e-01  5.28647661e-01]\n","Action space shape: 4\n","A sample action space: 1\n"]}],"source":["env = gym.make('LunarLander-v2')\n","env.reset()\n","\n","print(\"observation space shape: {}\".format(env.observation_space.shape))\n","print(\"A sample observation space: {}\".format(env.observation_space.sample()))\n","print('Action space shape: {}'.format(env.action_space.n))\n","print('A sample action space: {}'.format(env.action_space.sample()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsHanKID9n3v"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MHIvZaUH9n6v"},"source":["# Make vectorized environments\n","\n","* lets make 16 different environments for our model to train in so that it will diverse experience in each"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":807,"status":"ok","timestamp":1712848669877,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"lPjFikPJ12Np","outputId":"c9df1868-d2c4-4b55-9db3-6d94389531ff"},"outputs":[{"data":{"text/plain":["<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x79e5a744c280>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["env = make_vec_env('LunarLander-v2',n_envs=16)\n","env"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FA66KyXg12Qh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kXyv1Y_hHJ85"},"source":["# create the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":976,"status":"ok","timestamp":1712846760390,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"KLdv22QmMKUj","outputId":"6b371ef5-e962-4501-ff07-972a3c511508"},"outputs":[{"data":{"text/html":["<video src=\"https://cdn.openai.com/openai-baselines-ppo/knocked-over-stand-up.mp4\" controls  >\n","      Your browser does not support the <code>video</code> element.\n","    </video>"],"text/plain":["<IPython.core.display.Video object>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import Video\n","Video('https://cdn.openai.com/openai-baselines-ppo/knocked-over-stand-up.mp4')"]},{"cell_type":"markdown","metadata":{"id":"VUAAFWacLaIH"},"source":["\n","\n","* PPO lets us train AI policies in challenging environments, like the Roboschool one shown above where an agent tries to reach a target (the pink sphere), learning to walk, run, turn, use its momentum to recover from minor hits, and how to stand up from the ground when it is knocked over."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291833,"status":"ok","timestamp":1712849999641,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"tAHqPsoKHYW3","outputId":"8c85eaf1-f81c-4a27-8cd4-0b952bd2f77f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 95.7     |\n","|    ep_rew_mean     | -181     |\n","| time/              |          |\n","|    fps             | 542      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 1        |\n","|    total_timesteps | 1000     |\n","---------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 93.7        |\n","|    ep_rew_mean          | -203        |\n","| time/                   |             |\n","|    fps                  | 431         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 2000        |\n","| train/                  |             |\n","|    approx_kl            | 0.009256668 |\n","|    clip_fraction        | 0.0834      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.38       |\n","|    explained_variance   | 0.0173      |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 154         |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.00801    |\n","|    value_loss           | 1.04e+03    |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 95.5        |\n","|    ep_rew_mean          | -204        |\n","| time/                   |             |\n","|    fps                  | 406         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 3000        |\n","| train/                  |             |\n","|    approx_kl            | 0.009302775 |\n","|    clip_fraction        | 0.0819      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.36       |\n","|    explained_variance   | -0.084      |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 718         |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0124     |\n","|    value_loss           | 1.44e+03    |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 93.2        |\n","|    ep_rew_mean          | -188        |\n","| time/                   |             |\n","|    fps                  | 393         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 4000        |\n","| train/                  |             |\n","|    approx_kl            | 0.011271769 |\n","|    clip_fraction        | 0.0689      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.37       |\n","|    explained_variance   | 0.000193    |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 395         |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.00362    |\n","|    value_loss           | 1e+03       |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 95.7        |\n","|    ep_rew_mean          | -176        |\n","| time/                   |             |\n","|    fps                  | 372         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 5000        |\n","| train/                  |             |\n","|    approx_kl            | 0.015008191 |\n","|    clip_fraction        | 0.229       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.34       |\n","|    explained_variance   | 0.000833    |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 82.7        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0162     |\n","|    value_loss           | 402         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 95.2        |\n","|    ep_rew_mean          | -158        |\n","| time/                   |             |\n","|    fps                  | 370         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 16          |\n","|    total_timesteps      | 6000        |\n","| train/                  |             |\n","|    approx_kl            | 0.011000279 |\n","|    clip_fraction        | 0.13        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.31       |\n","|    explained_variance   | 0.0489      |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 55.3        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.0101     |\n","|    value_loss           | 247         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 96.4        |\n","|    ep_rew_mean          | -152        |\n","| time/                   |             |\n","|    fps                  | 368         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 18          |\n","|    total_timesteps      | 7000        |\n","| train/                  |             |\n","|    approx_kl            | 0.012443926 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.26       |\n","|    explained_variance   | 0.076       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 277         |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.00875    |\n","|    value_loss           | 341         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 97.4        |\n","|    ep_rew_mean          | -151        |\n","| time/                   |             |\n","|    fps                  | 368         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 21          |\n","|    total_timesteps      | 8000        |\n","| train/                  |             |\n","|    approx_kl            | 0.012685979 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.25       |\n","|    explained_variance   | 0.115       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 60.4        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.00597    |\n","|    value_loss           | 239         |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 97.8       |\n","|    ep_rew_mean          | -146       |\n","| time/                   |            |\n","|    fps                  | 362        |\n","|    iterations           | 9          |\n","|    time_elapsed         | 24         |\n","|    total_timesteps      | 9000       |\n","| train/                  |            |\n","|    approx_kl            | 0.02413388 |\n","|    clip_fraction        | 0.135      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.22      |\n","|    explained_variance   | -0.00598   |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 82.1       |\n","|    n_updates            | 80         |\n","|    policy_gradient_loss | -0.0156    |\n","|    value_loss           | 843        |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 103        |\n","|    ep_rew_mean          | -140       |\n","| time/                   |            |\n","|    fps                  | 360        |\n","|    iterations           | 10         |\n","|    time_elapsed         | 27         |\n","|    total_timesteps      | 10000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01199945 |\n","|    clip_fraction        | 0.216      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.17      |\n","|    explained_variance   | -0.0746    |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 59.9       |\n","|    n_updates            | 90         |\n","|    policy_gradient_loss | -0.0112    |\n","|    value_loss           | 164        |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 106         |\n","|    ep_rew_mean          | -132        |\n","| time/                   |             |\n","|    fps                  | 360         |\n","|    iterations           | 11          |\n","|    time_elapsed         | 30          |\n","|    total_timesteps      | 11000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011145059 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.21       |\n","|    explained_variance   | 0.24        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 50.6        |\n","|    n_updates            | 100         |\n","|    policy_gradient_loss | -0.0183     |\n","|    value_loss           | 213         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 110         |\n","|    ep_rew_mean          | -125        |\n","| time/                   |             |\n","|    fps                  | 360         |\n","|    iterations           | 12          |\n","|    time_elapsed         | 33          |\n","|    total_timesteps      | 12000       |\n","| train/                  |             |\n","|    approx_kl            | 0.017990293 |\n","|    clip_fraction        | 0.177       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.15       |\n","|    explained_variance   | 0.32        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 31.3        |\n","|    n_updates            | 110         |\n","|    policy_gradient_loss | -0.0127     |\n","|    value_loss           | 166         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 112         |\n","|    ep_rew_mean          | -118        |\n","| time/                   |             |\n","|    fps                  | 357         |\n","|    iterations           | 13          |\n","|    time_elapsed         | 36          |\n","|    total_timesteps      | 13000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010288042 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.18       |\n","|    explained_variance   | 0.394       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 57.5        |\n","|    n_updates            | 120         |\n","|    policy_gradient_loss | -0.0111     |\n","|    value_loss           | 275         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 123         |\n","|    ep_rew_mean          | -105        |\n","| time/                   |             |\n","|    fps                  | 355         |\n","|    iterations           | 14          |\n","|    time_elapsed         | 39          |\n","|    total_timesteps      | 14000       |\n","| train/                  |             |\n","|    approx_kl            | 0.024292681 |\n","|    clip_fraction        | 0.253       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.13       |\n","|    explained_variance   | 0.65        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 20.8        |\n","|    n_updates            | 130         |\n","|    policy_gradient_loss | -0.0247     |\n","|    value_loss           | 138         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 132         |\n","|    ep_rew_mean          | -103        |\n","| time/                   |             |\n","|    fps                  | 355         |\n","|    iterations           | 15          |\n","|    time_elapsed         | 42          |\n","|    total_timesteps      | 15000       |\n","| train/                  |             |\n","|    approx_kl            | 0.015153802 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.15       |\n","|    explained_variance   | 0.726       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 24.8        |\n","|    n_updates            | 140         |\n","|    policy_gradient_loss | -0.00645    |\n","|    value_loss           | 111         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 133         |\n","|    ep_rew_mean          | -100        |\n","| time/                   |             |\n","|    fps                  | 355         |\n","|    iterations           | 16          |\n","|    time_elapsed         | 45          |\n","|    total_timesteps      | 16000       |\n","| train/                  |             |\n","|    approx_kl            | 0.025910204 |\n","|    clip_fraction        | 0.24        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.17       |\n","|    explained_variance   | 0.676       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 28.2        |\n","|    n_updates            | 150         |\n","|    policy_gradient_loss | -0.00912    |\n","|    value_loss           | 66.2        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 146         |\n","|    ep_rew_mean          | -99.4       |\n","| time/                   |             |\n","|    fps                  | 353         |\n","|    iterations           | 17          |\n","|    time_elapsed         | 48          |\n","|    total_timesteps      | 17000       |\n","| train/                  |             |\n","|    approx_kl            | 0.008442526 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.09       |\n","|    explained_variance   | 0.557       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 62.9        |\n","|    n_updates            | 160         |\n","|    policy_gradient_loss | -0.00979    |\n","|    value_loss           | 138         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 155         |\n","|    ep_rew_mean          | -92.6       |\n","| time/                   |             |\n","|    fps                  | 351         |\n","|    iterations           | 18          |\n","|    time_elapsed         | 51          |\n","|    total_timesteps      | 18000       |\n","| train/                  |             |\n","|    approx_kl            | 0.019364197 |\n","|    clip_fraction        | 0.134       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.1        |\n","|    explained_variance   | 0.606       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 177         |\n","|    n_updates            | 170         |\n","|    policy_gradient_loss | -0.0137     |\n","|    value_loss           | 277         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 165         |\n","|    ep_rew_mean          | -89.8       |\n","| time/                   |             |\n","|    fps                  | 352         |\n","|    iterations           | 19          |\n","|    time_elapsed         | 53          |\n","|    total_timesteps      | 19000       |\n","| train/                  |             |\n","|    approx_kl            | 0.025502067 |\n","|    clip_fraction        | 0.305       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.19       |\n","|    explained_variance   | 0.823       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 16.9        |\n","|    n_updates            | 180         |\n","|    policy_gradient_loss | -0.0138     |\n","|    value_loss           | 30.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 169         |\n","|    ep_rew_mean          | -85.9       |\n","| time/                   |             |\n","|    fps                  | 352         |\n","|    iterations           | 20          |\n","|    time_elapsed         | 56          |\n","|    total_timesteps      | 20000       |\n","| train/                  |             |\n","|    approx_kl            | 0.021203753 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.09       |\n","|    explained_variance   | 0.792       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 9.18        |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.0127     |\n","|    value_loss           | 66.4        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 172          |\n","|    ep_rew_mean          | -84.9        |\n","| time/                   |              |\n","|    fps                  | 351          |\n","|    iterations           | 21           |\n","|    time_elapsed         | 59           |\n","|    total_timesteps      | 21000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0139321415 |\n","|    clip_fraction        | 0.157        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.02        |\n","|    explained_variance   | 0.299        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 10           |\n","|    n_updates            | 200          |\n","|    policy_gradient_loss | -0.00783     |\n","|    value_loss           | 54.7         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 182         |\n","|    ep_rew_mean          | -82.9       |\n","| time/                   |             |\n","|    fps                  | 350         |\n","|    iterations           | 22          |\n","|    time_elapsed         | 62          |\n","|    total_timesteps      | 22000       |\n","| train/                  |             |\n","|    approx_kl            | 0.021530122 |\n","|    clip_fraction        | 0.237       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.17       |\n","|    explained_variance   | 0.841       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 12.1        |\n","|    n_updates            | 210         |\n","|    policy_gradient_loss | -0.011      |\n","|    value_loss           | 49.6        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 193         |\n","|    ep_rew_mean          | -79         |\n","| time/                   |             |\n","|    fps                  | 351         |\n","|    iterations           | 23          |\n","|    time_elapsed         | 65          |\n","|    total_timesteps      | 23000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011794715 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.01       |\n","|    explained_variance   | 0.84        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 21          |\n","|    n_updates            | 220         |\n","|    policy_gradient_loss | -0.0142     |\n","|    value_loss           | 41.2        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 202         |\n","|    ep_rew_mean          | -76.8       |\n","| time/                   |             |\n","|    fps                  | 351         |\n","|    iterations           | 24          |\n","|    time_elapsed         | 68          |\n","|    total_timesteps      | 24000       |\n","| train/                  |             |\n","|    approx_kl            | 0.015098412 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.99       |\n","|    explained_variance   | 0.829       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 14.3        |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.0041     |\n","|    value_loss           | 60.5        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 212        |\n","|    ep_rew_mean          | -75.9      |\n","| time/                   |            |\n","|    fps                  | 351        |\n","|    iterations           | 25         |\n","|    time_elapsed         | 71         |\n","|    total_timesteps      | 25000      |\n","| train/                  |            |\n","|    approx_kl            | 0.02049752 |\n","|    clip_fraction        | 0.211      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.06      |\n","|    explained_variance   | 0.88       |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 25.6       |\n","|    n_updates            | 240        |\n","|    policy_gradient_loss | -0.0113    |\n","|    value_loss           | 39.4       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 221         |\n","|    ep_rew_mean          | -75.2       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 26          |\n","|    time_elapsed         | 74          |\n","|    total_timesteps      | 26000       |\n","| train/                  |             |\n","|    approx_kl            | 0.017264955 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.08       |\n","|    explained_variance   | 0.964       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 3.83        |\n","|    n_updates            | 250         |\n","|    policy_gradient_loss | -0.0142     |\n","|    value_loss           | 10.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 232         |\n","|    ep_rew_mean          | -72.6       |\n","| time/                   |             |\n","|    fps                  | 350         |\n","|    iterations           | 27          |\n","|    time_elapsed         | 77          |\n","|    total_timesteps      | 27000       |\n","| train/                  |             |\n","|    approx_kl            | 0.023263082 |\n","|    clip_fraction        | 0.249       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.05       |\n","|    explained_variance   | 0.939       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 3.49        |\n","|    n_updates            | 260         |\n","|    policy_gradient_loss | -0.00705    |\n","|    value_loss           | 22.6        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 241        |\n","|    ep_rew_mean          | -70.5      |\n","| time/                   |            |\n","|    fps                  | 350        |\n","|    iterations           | 28         |\n","|    time_elapsed         | 79         |\n","|    total_timesteps      | 28000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01378447 |\n","|    clip_fraction        | 0.147      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.03      |\n","|    explained_variance   | 0.914      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 1.78       |\n","|    n_updates            | 270        |\n","|    policy_gradient_loss | -0.0114    |\n","|    value_loss           | 48         |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 250         |\n","|    ep_rew_mean          | -67.7       |\n","| time/                   |             |\n","|    fps                  | 350         |\n","|    iterations           | 29          |\n","|    time_elapsed         | 82          |\n","|    total_timesteps      | 29000       |\n","| train/                  |             |\n","|    approx_kl            | 0.008978052 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.984      |\n","|    explained_variance   | 0.907       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 16.8        |\n","|    n_updates            | 280         |\n","|    policy_gradient_loss | -0.00642    |\n","|    value_loss           | 32.3        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 259          |\n","|    ep_rew_mean          | -64.9        |\n","| time/                   |              |\n","|    fps                  | 349          |\n","|    iterations           | 30           |\n","|    time_elapsed         | 85           |\n","|    total_timesteps      | 30000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0071190754 |\n","|    clip_fraction        | 0.155        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.965       |\n","|    explained_variance   | 0.965        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 7.42         |\n","|    n_updates            | 290          |\n","|    policy_gradient_loss | -0.00453     |\n","|    value_loss           | 11.6         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 269         |\n","|    ep_rew_mean          | -62.3       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 31          |\n","|    time_elapsed         | 88          |\n","|    total_timesteps      | 31000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011491015 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.02       |\n","|    explained_variance   | 0.981       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.72        |\n","|    n_updates            | 300         |\n","|    policy_gradient_loss | -0.00984    |\n","|    value_loss           | 7.79        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 270          |\n","|    ep_rew_mean          | -62.4        |\n","| time/                   |              |\n","|    fps                  | 349          |\n","|    iterations           | 32           |\n","|    time_elapsed         | 91           |\n","|    total_timesteps      | 32000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0076810196 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.01        |\n","|    explained_variance   | 0.842        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 111          |\n","|    n_updates            | 310          |\n","|    policy_gradient_loss | -0.0104      |\n","|    value_loss           | 143          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 282         |\n","|    ep_rew_mean          | -61.7       |\n","| time/                   |             |\n","|    fps                  | 350         |\n","|    iterations           | 33          |\n","|    time_elapsed         | 94          |\n","|    total_timesteps      | 33000       |\n","| train/                  |             |\n","|    approx_kl            | 0.012530663 |\n","|    clip_fraction        | 0.0957      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.889      |\n","|    explained_variance   | 0.887       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.51        |\n","|    n_updates            | 320         |\n","|    policy_gradient_loss | -0.00762    |\n","|    value_loss           | 43.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 286         |\n","|    ep_rew_mean          | -62.6       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 34          |\n","|    time_elapsed         | 97          |\n","|    total_timesteps      | 34000       |\n","| train/                  |             |\n","|    approx_kl            | 0.012391398 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.973      |\n","|    explained_variance   | 0.862       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 44.7        |\n","|    n_updates            | 330         |\n","|    policy_gradient_loss | -0.0111     |\n","|    value_loss           | 63.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 295         |\n","|    ep_rew_mean          | -60.3       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 35          |\n","|    time_elapsed         | 100         |\n","|    total_timesteps      | 35000       |\n","| train/                  |             |\n","|    approx_kl            | 0.014150009 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.884      |\n","|    explained_variance   | 0.881       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 53.4        |\n","|    n_updates            | 340         |\n","|    policy_gradient_loss | -0.00441    |\n","|    value_loss           | 71.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 304         |\n","|    ep_rew_mean          | -57.8       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 36          |\n","|    time_elapsed         | 103         |\n","|    total_timesteps      | 36000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010402587 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.01       |\n","|    explained_variance   | 0.973       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 11.8        |\n","|    n_updates            | 350         |\n","|    policy_gradient_loss | -0.00928    |\n","|    value_loss           | 16.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 318         |\n","|    ep_rew_mean          | -56.5       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 37          |\n","|    time_elapsed         | 105         |\n","|    total_timesteps      | 37000       |\n","| train/                  |             |\n","|    approx_kl            | 0.017717514 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.947      |\n","|    explained_variance   | 0.979       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.26        |\n","|    n_updates            | 360         |\n","|    policy_gradient_loss | -0.00201    |\n","|    value_loss           | 12.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 327         |\n","|    ep_rew_mean          | -55.5       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 38          |\n","|    time_elapsed         | 109         |\n","|    total_timesteps      | 38000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010666266 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.914      |\n","|    explained_variance   | 0.933       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 16.3        |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.0102     |\n","|    value_loss           | 48.6        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 336         |\n","|    ep_rew_mean          | -53.9       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 39          |\n","|    time_elapsed         | 111         |\n","|    total_timesteps      | 39000       |\n","| train/                  |             |\n","|    approx_kl            | 0.016977767 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.911      |\n","|    explained_variance   | 0.962       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 19.7        |\n","|    n_updates            | 380         |\n","|    policy_gradient_loss | -0.00839    |\n","|    value_loss           | 31.8        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 341         |\n","|    ep_rew_mean          | -55.1       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 40          |\n","|    time_elapsed         | 114         |\n","|    total_timesteps      | 40000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010198204 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.865      |\n","|    explained_variance   | 0.97        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.98        |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.00898    |\n","|    value_loss           | 9.6         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 352         |\n","|    ep_rew_mean          | -53.3       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 41          |\n","|    time_elapsed         | 117         |\n","|    total_timesteps      | 41000       |\n","| train/                  |             |\n","|    approx_kl            | 0.007909786 |\n","|    clip_fraction        | 0.0916      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.897      |\n","|    explained_variance   | 0.947       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 3.32        |\n","|    n_updates            | 400         |\n","|    policy_gradient_loss | -0.0033     |\n","|    value_loss           | 37.6        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 361         |\n","|    ep_rew_mean          | -50.3       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 42          |\n","|    time_elapsed         | 120         |\n","|    total_timesteps      | 42000       |\n","| train/                  |             |\n","|    approx_kl            | 0.012752548 |\n","|    clip_fraction        | 0.0903      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.918      |\n","|    explained_variance   | 0.959       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 125         |\n","|    n_updates            | 410         |\n","|    policy_gradient_loss | -0.000394   |\n","|    value_loss           | 34.3        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 367        |\n","|    ep_rew_mean          | -51.1      |\n","| time/                   |            |\n","|    fps                  | 348        |\n","|    iterations           | 43         |\n","|    time_elapsed         | 123        |\n","|    total_timesteps      | 43000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01788037 |\n","|    clip_fraction        | 0.127      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.884     |\n","|    explained_variance   | 0.974      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 0.971      |\n","|    n_updates            | 420        |\n","|    policy_gradient_loss | -0.00974   |\n","|    value_loss           | 9.14       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 380         |\n","|    ep_rew_mean          | -51.6       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 44          |\n","|    time_elapsed         | 126         |\n","|    total_timesteps      | 44000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011602456 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.987      |\n","|    explained_variance   | 0.961       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 62.3        |\n","|    n_updates            | 430         |\n","|    policy_gradient_loss | -0.0134     |\n","|    value_loss           | 29.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 389         |\n","|    ep_rew_mean          | -48.9       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 45          |\n","|    time_elapsed         | 128         |\n","|    total_timesteps      | 45000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010598573 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.867      |\n","|    explained_variance   | 0.936       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 114         |\n","|    n_updates            | 440         |\n","|    policy_gradient_loss | -0.00644    |\n","|    value_loss           | 86.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 397         |\n","|    ep_rew_mean          | -44.3       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 46          |\n","|    time_elapsed         | 132         |\n","|    total_timesteps      | 46000       |\n","| train/                  |             |\n","|    approx_kl            | 0.009742029 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.776      |\n","|    explained_variance   | 0.98        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.177       |\n","|    n_updates            | 450         |\n","|    policy_gradient_loss | -0.000165   |\n","|    value_loss           | 8.42        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 403         |\n","|    ep_rew_mean          | -43.4       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 47          |\n","|    time_elapsed         | 134         |\n","|    total_timesteps      | 47000       |\n","| train/                  |             |\n","|    approx_kl            | 0.038614824 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.782      |\n","|    explained_variance   | 0.694       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 25.9        |\n","|    n_updates            | 460         |\n","|    policy_gradient_loss | -0.0128     |\n","|    value_loss           | 319         |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 410          |\n","|    ep_rew_mean          | -43.1        |\n","| time/                   |              |\n","|    fps                  | 348          |\n","|    iterations           | 48           |\n","|    time_elapsed         | 137          |\n","|    total_timesteps      | 48000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0115107605 |\n","|    clip_fraction        | 0.0971       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.968       |\n","|    explained_variance   | 0.973        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 5.85         |\n","|    n_updates            | 470          |\n","|    policy_gradient_loss | -0.00699     |\n","|    value_loss           | 28.8         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 416         |\n","|    ep_rew_mean          | -42.6       |\n","| time/                   |             |\n","|    fps                  | 349         |\n","|    iterations           | 49          |\n","|    time_elapsed         | 140         |\n","|    total_timesteps      | 49000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011268531 |\n","|    clip_fraction        | 0.0948      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.952      |\n","|    explained_variance   | 0.976       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.8         |\n","|    n_updates            | 480         |\n","|    policy_gradient_loss | -0.00569    |\n","|    value_loss           | 24.5        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 425         |\n","|    ep_rew_mean          | -40.3       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 50          |\n","|    time_elapsed         | 143         |\n","|    total_timesteps      | 50000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010518997 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.975      |\n","|    explained_variance   | 0.987       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 2           |\n","|    n_updates            | 490         |\n","|    policy_gradient_loss | -0.00699    |\n","|    value_loss           | 19.2        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 432         |\n","|    ep_rew_mean          | -32.7       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 51          |\n","|    time_elapsed         | 146         |\n","|    total_timesteps      | 51000       |\n","| train/                  |             |\n","|    approx_kl            | 0.014413003 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.878      |\n","|    explained_variance   | 0.967       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 2.45        |\n","|    n_updates            | 500         |\n","|    policy_gradient_loss | 0.00226     |\n","|    value_loss           | 9.6         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 440         |\n","|    ep_rew_mean          | -32.5       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 52          |\n","|    time_elapsed         | 149         |\n","|    total_timesteps      | 52000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011537924 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.844      |\n","|    explained_variance   | 0.812       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 6.2         |\n","|    n_updates            | 510         |\n","|    policy_gradient_loss | -0.00876    |\n","|    value_loss           | 218         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 449         |\n","|    ep_rew_mean          | -31.2       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 53          |\n","|    time_elapsed         | 152         |\n","|    total_timesteps      | 53000       |\n","| train/                  |             |\n","|    approx_kl            | 0.013299912 |\n","|    clip_fraction        | 0.145       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.935      |\n","|    explained_variance   | 0.983       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 6.41        |\n","|    n_updates            | 520         |\n","|    policy_gradient_loss | -0.00985    |\n","|    value_loss           | 25.8        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 458         |\n","|    ep_rew_mean          | -28.9       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 54          |\n","|    time_elapsed         | 155         |\n","|    total_timesteps      | 54000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011922531 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.757      |\n","|    explained_variance   | 0.815       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.75        |\n","|    n_updates            | 530         |\n","|    policy_gradient_loss | -0.003      |\n","|    value_loss           | 37.3        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 467        |\n","|    ep_rew_mean          | -26.4      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 55         |\n","|    time_elapsed         | 158        |\n","|    total_timesteps      | 55000      |\n","| train/                  |            |\n","|    approx_kl            | 0.02610618 |\n","|    clip_fraction        | 0.163      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.899     |\n","|    explained_variance   | 0.746      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 20.2       |\n","|    n_updates            | 540        |\n","|    policy_gradient_loss | -0.0108    |\n","|    value_loss           | 297        |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 476         |\n","|    ep_rew_mean          | -25         |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 56          |\n","|    time_elapsed         | 160         |\n","|    total_timesteps      | 56000       |\n","| train/                  |             |\n","|    approx_kl            | 0.022436421 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.798      |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 5.64        |\n","|    n_updates            | 550         |\n","|    policy_gradient_loss | -0.00421    |\n","|    value_loss           | 17.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 482         |\n","|    ep_rew_mean          | -24.5       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 57          |\n","|    time_elapsed         | 163         |\n","|    total_timesteps      | 57000       |\n","| train/                  |             |\n","|    approx_kl            | 0.020072747 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.752      |\n","|    explained_variance   | 0.967       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 3.07        |\n","|    n_updates            | 560         |\n","|    policy_gradient_loss | -0.000472   |\n","|    value_loss           | 6.74        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 486         |\n","|    ep_rew_mean          | -24.8       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 58          |\n","|    time_elapsed         | 166         |\n","|    total_timesteps      | 58000       |\n","| train/                  |             |\n","|    approx_kl            | 0.006777247 |\n","|    clip_fraction        | 0.0855      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.914      |\n","|    explained_variance   | 0.868       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 4.75        |\n","|    n_updates            | 570         |\n","|    policy_gradient_loss | -0.00282    |\n","|    value_loss           | 36.1        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 495          |\n","|    ep_rew_mean          | -23.2        |\n","| time/                   |              |\n","|    fps                  | 347          |\n","|    iterations           | 59           |\n","|    time_elapsed         | 169          |\n","|    total_timesteps      | 59000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0037185333 |\n","|    clip_fraction        | 0.0476       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.946       |\n","|    explained_variance   | 0.933        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 3.21         |\n","|    n_updates            | 580          |\n","|    policy_gradient_loss | -0.00319     |\n","|    value_loss           | 34.8         |\n","------------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 503        |\n","|    ep_rew_mean          | -25.5      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 60         |\n","|    time_elapsed         | 172        |\n","|    total_timesteps      | 60000      |\n","| train/                  |            |\n","|    approx_kl            | 0.02573894 |\n","|    clip_fraction        | 0.17       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.926     |\n","|    explained_variance   | 0.946      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 2.04       |\n","|    n_updates            | 590        |\n","|    policy_gradient_loss | -0.00728   |\n","|    value_loss           | 6.09       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 510         |\n","|    ep_rew_mean          | -26.4       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 61          |\n","|    time_elapsed         | 175         |\n","|    total_timesteps      | 61000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010236864 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.995      |\n","|    explained_variance   | 0.967       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 5.63        |\n","|    n_updates            | 600         |\n","|    policy_gradient_loss | -0.00476    |\n","|    value_loss           | 9.87        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 513        |\n","|    ep_rew_mean          | -24.9      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 62         |\n","|    time_elapsed         | 178        |\n","|    total_timesteps      | 62000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01108242 |\n","|    clip_fraction        | 0.144      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.888     |\n","|    explained_variance   | 0.834      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 38.1       |\n","|    n_updates            | 610        |\n","|    policy_gradient_loss | -0.00951   |\n","|    value_loss           | 25.2       |\n","----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 522          |\n","|    ep_rew_mean          | -26.3        |\n","| time/                   |              |\n","|    fps                  | 347          |\n","|    iterations           | 63           |\n","|    time_elapsed         | 181          |\n","|    total_timesteps      | 63000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0125002405 |\n","|    clip_fraction        | 0.132        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.921       |\n","|    explained_variance   | 0.937        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 8.69         |\n","|    n_updates            | 620          |\n","|    policy_gradient_loss | -0.00357     |\n","|    value_loss           | 15.3         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 530         |\n","|    ep_rew_mean          | -27.6       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 64          |\n","|    time_elapsed         | 183         |\n","|    total_timesteps      | 64000       |\n","| train/                  |             |\n","|    approx_kl            | 0.015728034 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.03       |\n","|    explained_variance   | 0.964       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 3.58        |\n","|    n_updates            | 630         |\n","|    policy_gradient_loss | -0.00398    |\n","|    value_loss           | 8.17        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 538         |\n","|    ep_rew_mean          | -28.8       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 65          |\n","|    time_elapsed         | 186         |\n","|    total_timesteps      | 65000       |\n","| train/                  |             |\n","|    approx_kl            | 0.025354812 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.883      |\n","|    explained_variance   | 0.966       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.231       |\n","|    n_updates            | 640         |\n","|    policy_gradient_loss | -0.00607    |\n","|    value_loss           | 10.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 542         |\n","|    ep_rew_mean          | -27.5       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 66          |\n","|    time_elapsed         | 189         |\n","|    total_timesteps      | 66000       |\n","| train/                  |             |\n","|    approx_kl            | 0.013893472 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.01       |\n","|    explained_variance   | 0.925       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.28        |\n","|    n_updates            | 650         |\n","|    policy_gradient_loss | -0.0113     |\n","|    value_loss           | 16.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 557         |\n","|    ep_rew_mean          | -27.4       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 67          |\n","|    time_elapsed         | 192         |\n","|    total_timesteps      | 67000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011274284 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.975      |\n","|    explained_variance   | 0.372       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 31.3        |\n","|    n_updates            | 660         |\n","|    policy_gradient_loss | -0.00288    |\n","|    value_loss           | 238         |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 564        |\n","|    ep_rew_mean          | -28.9      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 68         |\n","|    time_elapsed         | 195        |\n","|    total_timesteps      | 68000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01213808 |\n","|    clip_fraction        | 0.0996     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.986     |\n","|    explained_variance   | 0.885      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 1.4        |\n","|    n_updates            | 670        |\n","|    policy_gradient_loss | -0.00528   |\n","|    value_loss           | 32.2       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 569         |\n","|    ep_rew_mean          | -27.5       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 69          |\n","|    time_elapsed         | 198         |\n","|    total_timesteps      | 69000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011275988 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.05       |\n","|    explained_variance   | 0.978       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.92        |\n","|    n_updates            | 680         |\n","|    policy_gradient_loss | -0.00286    |\n","|    value_loss           | 9.93        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 577         |\n","|    ep_rew_mean          | -26.6       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 70          |\n","|    time_elapsed         | 201         |\n","|    total_timesteps      | 70000       |\n","| train/                  |             |\n","|    approx_kl            | 0.008163585 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.972      |\n","|    explained_variance   | 0.549       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 8.63        |\n","|    n_updates            | 690         |\n","|    policy_gradient_loss | -0.00306    |\n","|    value_loss           | 190         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 585         |\n","|    ep_rew_mean          | -26.4       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 71          |\n","|    time_elapsed         | 204         |\n","|    total_timesteps      | 71000       |\n","| train/                  |             |\n","|    approx_kl            | 0.022064742 |\n","|    clip_fraction        | 0.212       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.979      |\n","|    explained_variance   | 0.859       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.8         |\n","|    n_updates            | 700         |\n","|    policy_gradient_loss | -0.013      |\n","|    value_loss           | 14.8        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 596        |\n","|    ep_rew_mean          | -26.1      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 72         |\n","|    time_elapsed         | 207        |\n","|    total_timesteps      | 72000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01771706 |\n","|    clip_fraction        | 0.254      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.941     |\n","|    explained_variance   | 0.961      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 13.3       |\n","|    n_updates            | 710        |\n","|    policy_gradient_loss | -0.00727   |\n","|    value_loss           | 6.65       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 605         |\n","|    ep_rew_mean          | -24.6       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 73          |\n","|    time_elapsed         | 209         |\n","|    total_timesteps      | 73000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010948185 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.986      |\n","|    explained_variance   | 0.956       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.185       |\n","|    n_updates            | 720         |\n","|    policy_gradient_loss | 0.00118     |\n","|    value_loss           | 4.53        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 613         |\n","|    ep_rew_mean          | -23.4       |\n","| time/                   |             |\n","|    fps                  | 348         |\n","|    iterations           | 74          |\n","|    time_elapsed         | 212         |\n","|    total_timesteps      | 74000       |\n","| train/                  |             |\n","|    approx_kl            | 0.019608948 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.97       |\n","|    explained_variance   | 0.944       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.254       |\n","|    n_updates            | 730         |\n","|    policy_gradient_loss | -0.00332    |\n","|    value_loss           | 8.22        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 613         |\n","|    ep_rew_mean          | -23.7       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 75          |\n","|    time_elapsed         | 215         |\n","|    total_timesteps      | 75000       |\n","| train/                  |             |\n","|    approx_kl            | 0.018037304 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.01       |\n","|    explained_variance   | 0.899       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 2.11        |\n","|    n_updates            | 740         |\n","|    policy_gradient_loss | -0.00663    |\n","|    value_loss           | 12.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 622         |\n","|    ep_rew_mean          | -22.8       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 76          |\n","|    time_elapsed         | 218         |\n","|    total_timesteps      | 76000       |\n","| train/                  |             |\n","|    approx_kl            | 0.015344784 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.975      |\n","|    explained_variance   | 0.95        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.95        |\n","|    n_updates            | 750         |\n","|    policy_gradient_loss | -0.00394    |\n","|    value_loss           | 5.99        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 624        |\n","|    ep_rew_mean          | -24.1      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 77         |\n","|    time_elapsed         | 221        |\n","|    total_timesteps      | 77000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01647842 |\n","|    clip_fraction        | 0.192      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.926     |\n","|    explained_variance   | 0.92       |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 0.142      |\n","|    n_updates            | 760        |\n","|    policy_gradient_loss | -0.00981   |\n","|    value_loss           | 5.08       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 633         |\n","|    ep_rew_mean          | -23.3       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 78          |\n","|    time_elapsed         | 224         |\n","|    total_timesteps      | 78000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011405351 |\n","|    clip_fraction        | 0.0902      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.02       |\n","|    explained_variance   | 0.782       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.254       |\n","|    n_updates            | 770         |\n","|    policy_gradient_loss | -0.00873    |\n","|    value_loss           | 16          |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 633         |\n","|    ep_rew_mean          | -23.3       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 79          |\n","|    time_elapsed         | 227         |\n","|    total_timesteps      | 79000       |\n","| train/                  |             |\n","|    approx_kl            | 0.014415104 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.929      |\n","|    explained_variance   | 0.972       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.0622      |\n","|    n_updates            | 780         |\n","|    policy_gradient_loss | 0.00288     |\n","|    value_loss           | 2.38        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 641         |\n","|    ep_rew_mean          | -23.2       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 80          |\n","|    time_elapsed         | 230         |\n","|    total_timesteps      | 80000       |\n","| train/                  |             |\n","|    approx_kl            | 0.013874362 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.904      |\n","|    explained_variance   | 0.865       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 4.26        |\n","|    n_updates            | 790         |\n","|    policy_gradient_loss | -0.00936    |\n","|    value_loss           | 15.9        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 649          |\n","|    ep_rew_mean          | -22.5        |\n","| time/                   |              |\n","|    fps                  | 347          |\n","|    iterations           | 81           |\n","|    time_elapsed         | 232          |\n","|    total_timesteps      | 81000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0067943987 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.99        |\n","|    explained_variance   | 0.87         |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 1.91         |\n","|    n_updates            | 800          |\n","|    policy_gradient_loss | -0.0083      |\n","|    value_loss           | 8.81         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 655         |\n","|    ep_rew_mean          | -20.2       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 82          |\n","|    time_elapsed         | 235         |\n","|    total_timesteps      | 82000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010263369 |\n","|    clip_fraction        | 0.0783      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.95       |\n","|    explained_variance   | 0.337       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.88        |\n","|    n_updates            | 810         |\n","|    policy_gradient_loss | -0.000629   |\n","|    value_loss           | 155         |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 657        |\n","|    ep_rew_mean          | -18.5      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 83         |\n","|    time_elapsed         | 238        |\n","|    total_timesteps      | 83000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01810399 |\n","|    clip_fraction        | 0.186      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.929     |\n","|    explained_variance   | 0.558      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 17.2       |\n","|    n_updates            | 820        |\n","|    policy_gradient_loss | -0.00909   |\n","|    value_loss           | 316        |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 651         |\n","|    ep_rew_mean          | -20.2       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 84          |\n","|    time_elapsed         | 241         |\n","|    total_timesteps      | 84000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011099205 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.978      |\n","|    explained_variance   | 0.929       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.69        |\n","|    n_updates            | 830         |\n","|    policy_gradient_loss | -0.00467    |\n","|    value_loss           | 10.3        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 659         |\n","|    ep_rew_mean          | -19.1       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 85          |\n","|    time_elapsed         | 244         |\n","|    total_timesteps      | 85000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011598034 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.972      |\n","|    explained_variance   | 0.912       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 8.62        |\n","|    n_updates            | 840         |\n","|    policy_gradient_loss | -0.00356    |\n","|    value_loss           | 11.7        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 667         |\n","|    ep_rew_mean          | -18.1       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 86          |\n","|    time_elapsed         | 247         |\n","|    total_timesteps      | 86000       |\n","| train/                  |             |\n","|    approx_kl            | 0.008484086 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.996      |\n","|    explained_variance   | 0.975       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.446       |\n","|    n_updates            | 850         |\n","|    policy_gradient_loss | -0.00816    |\n","|    value_loss           | 2.81        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 672         |\n","|    ep_rew_mean          | -16.9       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 87          |\n","|    time_elapsed         | 250         |\n","|    total_timesteps      | 87000       |\n","| train/                  |             |\n","|    approx_kl            | 0.016252376 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.01       |\n","|    explained_variance   | 0.982       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.11        |\n","|    n_updates            | 860         |\n","|    policy_gradient_loss | -0.004      |\n","|    value_loss           | 1.47        |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 678        |\n","|    ep_rew_mean          | -14.8      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 88         |\n","|    time_elapsed         | 253        |\n","|    total_timesteps      | 88000      |\n","| train/                  |            |\n","|    approx_kl            | 0.01219508 |\n","|    clip_fraction        | 0.175      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.914     |\n","|    explained_variance   | 0.896      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 8.9        |\n","|    n_updates            | 870        |\n","|    policy_gradient_loss | -0.00927   |\n","|    value_loss           | 12.9       |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 678        |\n","|    ep_rew_mean          | -14.6      |\n","| time/                   |            |\n","|    fps                  | 347        |\n","|    iterations           | 89         |\n","|    time_elapsed         | 256        |\n","|    total_timesteps      | 89000      |\n","| train/                  |            |\n","|    approx_kl            | 0.07963407 |\n","|    clip_fraction        | 0.3        |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.929     |\n","|    explained_variance   | 0.933      |\n","|    learning_rate        | 0.001      |\n","|    loss                 | 7.89       |\n","|    n_updates            | 880        |\n","|    policy_gradient_loss | -0.0249    |\n","|    value_loss           | 6.54       |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 678         |\n","|    ep_rew_mean          | -13.9       |\n","| time/                   |             |\n","|    fps                  | 347         |\n","|    iterations           | 90          |\n","|    time_elapsed         | 259         |\n","|    total_timesteps      | 90000       |\n","| train/                  |             |\n","|    approx_kl            | 0.024431363 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.06       |\n","|    explained_variance   | 0.972       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.26        |\n","|    n_updates            | 890         |\n","|    policy_gradient_loss | -0.00406    |\n","|    value_loss           | 2.88        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 685          |\n","|    ep_rew_mean          | -12.2        |\n","| time/                   |              |\n","|    fps                  | 346          |\n","|    iterations           | 91           |\n","|    time_elapsed         | 262          |\n","|    total_timesteps      | 91000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0046710153 |\n","|    clip_fraction        | 0.0784       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.98        |\n","|    explained_variance   | 0.917        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 1.89         |\n","|    n_updates            | 900          |\n","|    policy_gradient_loss | -0.00592     |\n","|    value_loss           | 7.63         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 678         |\n","|    ep_rew_mean          | -10.6       |\n","| time/                   |             |\n","|    fps                  | 346         |\n","|    iterations           | 92          |\n","|    time_elapsed         | 265         |\n","|    total_timesteps      | 92000       |\n","| train/                  |             |\n","|    approx_kl            | 0.044546567 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.05       |\n","|    explained_variance   | 0.973       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.441       |\n","|    n_updates            | 910         |\n","|    policy_gradient_loss | -0.00396    |\n","|    value_loss           | 2.27        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 678          |\n","|    ep_rew_mean          | -10.1        |\n","| time/                   |              |\n","|    fps                  | 346          |\n","|    iterations           | 93           |\n","|    time_elapsed         | 268          |\n","|    total_timesteps      | 93000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0033798958 |\n","|    clip_fraction        | 0.0406       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.864       |\n","|    explained_variance   | 0.451        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 0.997        |\n","|    n_updates            | 920          |\n","|    policy_gradient_loss | -0.00203     |\n","|    value_loss           | 212          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 678         |\n","|    ep_rew_mean          | -9.36       |\n","| time/                   |             |\n","|    fps                  | 346         |\n","|    iterations           | 94          |\n","|    time_elapsed         | 271         |\n","|    total_timesteps      | 94000       |\n","| train/                  |             |\n","|    approx_kl            | 0.013621563 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.992      |\n","|    explained_variance   | 0.98        |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.833       |\n","|    n_updates            | 930         |\n","|    policy_gradient_loss | -0.00524    |\n","|    value_loss           | 2.94        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 678         |\n","|    ep_rew_mean          | -9.43       |\n","| time/                   |             |\n","|    fps                  | 345         |\n","|    iterations           | 95          |\n","|    time_elapsed         | 275         |\n","|    total_timesteps      | 95000       |\n","| train/                  |             |\n","|    approx_kl            | 0.016799204 |\n","|    clip_fraction        | 0.201       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.11       |\n","|    explained_variance   | 0.982       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.523       |\n","|    n_updates            | 940         |\n","|    policy_gradient_loss | -0.00725    |\n","|    value_loss           | 2.9         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 685         |\n","|    ep_rew_mean          | -7.76       |\n","| time/                   |             |\n","|    fps                  | 345         |\n","|    iterations           | 96          |\n","|    time_elapsed         | 277         |\n","|    total_timesteps      | 96000       |\n","| train/                  |             |\n","|    approx_kl            | 0.010860967 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.06       |\n","|    explained_variance   | 0.984       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.28        |\n","|    n_updates            | 950         |\n","|    policy_gradient_loss | -0.00433    |\n","|    value_loss           | 2.92        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 671         |\n","|    ep_rew_mean          | -11.5       |\n","| time/                   |             |\n","|    fps                  | 345         |\n","|    iterations           | 97          |\n","|    time_elapsed         | 280         |\n","|    total_timesteps      | 97000       |\n","| train/                  |             |\n","|    approx_kl            | 0.014511668 |\n","|    clip_fraction        | 0.149       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.837      |\n","|    explained_variance   | 0.978       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.17        |\n","|    n_updates            | 960         |\n","|    policy_gradient_loss | -0.0103     |\n","|    value_loss           | 2.62        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 674          |\n","|    ep_rew_mean          | -8.75        |\n","| time/                   |              |\n","|    fps                  | 345          |\n","|    iterations           | 98           |\n","|    time_elapsed         | 283          |\n","|    total_timesteps      | 98000        |\n","| train/                  |              |\n","|    approx_kl            | 0.0057814466 |\n","|    clip_fraction        | 0.0558       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.913       |\n","|    explained_variance   | 0.916        |\n","|    learning_rate        | 0.001        |\n","|    loss                 | 1.17         |\n","|    n_updates            | 970          |\n","|    policy_gradient_loss | -0.00331     |\n","|    value_loss           | 39.6         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 683         |\n","|    ep_rew_mean          | -6.51       |\n","| time/                   |             |\n","|    fps                  | 345         |\n","|    iterations           | 99          |\n","|    time_elapsed         | 286         |\n","|    total_timesteps      | 99000       |\n","| train/                  |             |\n","|    approx_kl            | 0.011031836 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.09       |\n","|    explained_variance   | 0.968       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 1.83        |\n","|    n_updates            | 980         |\n","|    policy_gradient_loss | -0.00412    |\n","|    value_loss           | 4.2         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 689         |\n","|    ep_rew_mean          | -4.66       |\n","| time/                   |             |\n","|    fps                  | 345         |\n","|    iterations           | 100         |\n","|    time_elapsed         | 289         |\n","|    total_timesteps      | 100000      |\n","| train/                  |             |\n","|    approx_kl            | 0.017052267 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.883      |\n","|    explained_variance   | 0.993       |\n","|    learning_rate        | 0.001       |\n","|    loss                 | 0.464       |\n","|    n_updates            | 990         |\n","|    policy_gradient_loss | -0.000122   |\n","|    value_loss           | 1.46        |\n","-----------------------------------------\n"]}],"source":["# create the environment\n","import gymnasium as gym\n","env = gym.make('LunarLander-v2')\n","\n","# instantiate the Proximal Policy Optimization model (agent)\n","\"\"\"\n","MlpPolicy is Multi Layer Perceptron Neural network.\n","If we had image frames we would have used CnnPolicy\n","\"\"\"\n","\n","# creating the agent\n","model = PPO('MlpPolicy', # MLP is multi layer perceptron\n","            env,\n","            verbose=1,\n","            learning_rate=0.001,\n","            n_steps=2500,\n","            batch_size=32,\n","            n_epochs=10,\n","            gamma=0.999, # discount factor.\n","            gae_lambda=0.95, # generalized advantage estimator (a tradeoff between bias vs variance) used for generalization purposes.\n","            ent_coef=0.01 # entropy coefficient for loss calculation.\n","            )\n","\n","model.learn(total_timesteps=100000)\n","\n","# lets save the model\n","model_name = 'PPO_LunarLander-v2'\n","model.save(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4245,"status":"ok","timestamp":1712850003838,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"XRjS8F71HYZo","outputId":"494de9b2-a7a9-4c71-8965-57eafd3664f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean reward: 137.22, Std reward: 98.53\n"]}],"source":["# eval env\n","eval_env = Monitor(gym.make('LunarLander-v2'))\n","\n","# evaluate the model/agent\n","\"\"\"\n","what n_eval_episodes does?\n"," It essentially controls how many times the agent will interact with the environment to assess its performance.\n"," \"\"\"\n","mean_reward, std_reward = evaluate_policy(model,eval_env,n_eval_episodes=10,deterministic=True)\n","\n","print('Mean reward: {:.2f}, Std reward: {:.2f}'.format(mean_reward,std_reward))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsasE1cbHYcq"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3Ra9L8YjHUxG"},"source":["# uploading the model to huggingface hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68w8sfiLHUzs"},"outputs":[],"source":["from huggingface_hub import notebook_login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d30d3e95b6854e55affc55c71f231962","b6189fe196944332bc70f2c694797626","f31107e733044104ad975642193635c1","3a4d8d672cb745a2ab2a7b88617fecec","6518fca7c89a4dcab4d2a8cb4fe7c2f0","a8d5f8060cbd4e439b8205ac02779414","090327cc3d9c41829c87b4b02dd6c1f3","54b904886b794392ba60cced4f3ab7b7","518d48ea068e4494acead0ba36f9ee94","e4fe14bb913e40b8a7447fe62a3ac963","43e2055a262b458388f34f2993deb0aa","492fe7442f894b70b69d516f60827a93","93cad192bf93422bbbd84658d1894220","630db6b4fbf248369882e1c816865899"]},"executionInfo":{"elapsed":832,"status":"ok","timestamp":1712678113580,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"KeSGB863kfJ_","outputId":"da3785ff-6087-462c-ac14-111a3e7f5fee"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d30d3e95b6854e55affc55c71f231962","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646,"referenced_widgets":["952f0bb1e50f453a9edd0ee0e33b8e62","fc87b4f7e3654f4ea4088a7dd678c3fa","03e13db6a3c24f64b70f850dca92322d","1f01c451c1a3403caf3906d8ea018d47","1a0715148dc24fcd95aff806597c8bdd","b10e2e10e80c4bf1a2cf7435f381ab2a","8df0ba72727c4799829504db9b3cae54","77a50e308d9149de82cfef44e8fdafcc","62a8e31fcb4f4ebba9f2b86782f88250","96b81afe46c6462f90f42fd03fffeb9a","f65130317f904568af88e977c26c2f99","54f0f6eb69c949e69ce834a6d170b767","029afece5a8049f89bf4cd535ea78bf8","88a6b08a618e409c896d1579ea973056","00220cb4e429457895ec7653d1d2f613","3c340e60a372495db0c6cc64563a3816","0a0f34697cf7449c9498f8b72860bef0","9ee2e49efa5b4a79a413809205d8790f","8c5733a85f174426870037bd4e314c54","71b3a711598c4b538f57b0c941d15734","33aba1e32ad148aa88cad78cc766b07b","fbce52ebe9564935bc0ee6f9b5f87e6f","eb6f7ec9dc914c3a9772935a643e0e1c","f1e34ebd19cc47d78b3916d597447393","a39fb3c07ab74d409512a8205da77fbe","5e5e0523fb654afa9dbaea6d41b03452","410f517dc8374ad49d7f6e82be12e42b","a60fb77298244c2d98229e1478662317","1b2a589ce2204477aa957fd02f912f11","48583169d0304dbc9f115d1f24509d78","dd70926e67654447bcd051a5b7e44188","3a87a7ea8f9e436b9994a9dbb5a83d78","56ccab345a474dd2a5d08c005c553624","70313a3c9c4a4ea4bb393fb79acef9ad","9b726a2956bc421789eb0aaa6c089e35","4003ea162ddc44559e0c977f0adaecc1","0200f0c5465c481da5c31aa9b788803e","21cd05becd4d4b2faa7f7174c8dc97ad","c00af2b55f1b49768f168aa9c633ee16","409c4c28f0e44c1d93f99ae8e85e03a4","ee427dcae90141d0ae20a40503a3afd4","13e5f9934f8f4ce3ae2e7a345c41cad3","4d58646fce514d7a9793356c08f41a49","53d73ae308694e75b2920d908d874d16","a3bf406f9233420ea1da3ec4284bc652","c528df39478b43cbb5df852e73b8f29e","70052eb56e604c1ab61d386d7169190d","197de12f31784dc482f26a6a0af54c83","c28aa1f610b443f1b8c24743a59bc41d","28663c1ad69f4403a4dfccbbd4f7d61b","e9653923f56747b58ddf20f02ee891b9","f0f04468fb3047b2ab5713d5735e312a","4cbbbc7f0a1b4b59b731087915031863","0b1949c65ea642b4a79517850bbe8b48","5f305e4f4d134078a5da409732624d8b"]},"executionInfo":{"elapsed":19584,"status":"ok","timestamp":1712679089890,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"0zLKJiMoiWLA","outputId":"f1627149-752c-411e-fa80-095eca462ff1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[38;5;4mℹ This function will save, evaluate, generate a video of your agent,\n","create a model card and push everything to the hub. It might take up to 1min.\n","This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function VecVideoRecorder.__del__ at 0x7aa33a48f6d0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\", line 113, in __del__\n","    self.close_video_recorder()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\", line 103, in close_video_recorder\n","    if self.recording:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\", line 360, in __getattr__\n","    return self.getattr_recursive(name)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\", line 385, in getattr_recursive\n","    attr = getattr(self.venv, name)\n","AttributeError: 'DummyVecEnv' object has no attribute 'recording'\n"]},{"name":"stdout","output_type":"stream","text":["Saving video to /tmp/tmpngc2qigd/-step-0-to-step-1000.mp4\n","Moviepy - Building video /tmp/tmpngc2qigd/-step-0-to-step-1000.mp4.\n","Moviepy - Writing video /tmp/tmpngc2qigd/-step-0-to-step-1000.mp4\n","\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Moviepy - Done !\n","Moviepy - video ready /tmp/tmpngc2qigd/-step-0-to-step-1000.mp4\n","\u001b[38;5;4mℹ Pushing repo GeorgeImmanuel/PPO_LunarLander-v2 to the Hugging Face\n","Hub\u001b[0m\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"952f0bb1e50f453a9edd0ee0e33b8e62","version_major":2,"version_minor":0},"text/plain":["policy.optimizer.pth:   0%|          | 0.00/88.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54f0f6eb69c949e69ce834a6d170b767","version_major":2,"version_minor":0},"text/plain":["policy.pth:   0%|          | 0.00/43.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb6f7ec9dc914c3a9772935a643e0e1c","version_major":2,"version_minor":0},"text/plain":["Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70313a3c9c4a4ea4bb393fb79acef9ad","version_major":2,"version_minor":0},"text/plain":["pytorch_variables.pth:   0%|          | 0.00/864 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3bf406f9233420ea1da3ec4284bc652","version_major":2,"version_minor":0},"text/plain":["PPO_LunarLander-v2.zip:   0%|          | 0.00/147k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[38;5;4mℹ Your model is pushed to the Hub. You can view your model here:\n","https://huggingface.co/GeorgeImmanuel/PPO_LunarLander-v2/tree/main/\u001b[0m\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["CommitInfo(commit_url='https://huggingface.co/GeorgeImmanuel/PPO_LunarLander-v2/commit/7265fe673d031796f066afdfd5ba9587322e6fc3', commit_message='upload PPO_LunarLander-v2 trained agent! ', commit_description='', oid='7265fe673d031796f066afdfd5ba9587322e6fc3', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from stable_baselines3.common.vec_env import DummyVecEnv\n","\n","env_id = 'LunarLander-v2'\n","eval_env = DummyVecEnv([lambda:Monitor(gym.make(env_id,render_mode='rgb_array'))])\n","\n","# to save and evaluate our model in huggingface.\n","package_to_hub(model=model,\n","               model_name=model_name,\n","               model_architecture='PPO',\n","               env_id='LunarLander-v2',\n","               eval_env=eval_env,\n","               repo_id='GeorgeImmanuel/PPO_LunarLander-v2',\n","               commit_message='upload PPO_LunarLander-v2 trained agent! ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLPtnWSEiWNs"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"p9TVtxYHpBGq"},"source":["# load from huggingface hub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5954,"status":"ok","timestamp":1712679799203,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"NTF-vqespBJh","outputId":"c56b5986-084a-49d3-8796-ffd6dd5502ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting shimmy\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.25.2)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (0.28.1)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (4.10.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install shimmy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":431,"referenced_widgets":["b1e5c4f3967740518897967125d41678","753203c47baf46dd9b0a39bb48c8d093","b3de8947c1ef4960aefbb84a109d1c78","a0ca8500e1a342958730a630be636a46","9f0a13718c0a4e1eae39f49e5153a5d5","135538f49f6a488e8ad63e9210add5dd","865fa67ba3f74344a7a1ed10fd18399d","e9838854dbab4f3cab812a45a784e73e","dd0e4ff07b494b4594a0bd1495e63784","e9cbf25d0ccd43639fd50c98086d5c80","e538d432507743dcb0e8809a5a0a60f3"]},"executionInfo":{"elapsed":2105,"status":"ok","timestamp":1712680109553,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"YOtKa29RiWQd","outputId":"c48062fc-42c2-4a26-fd60-adc4c4040fe8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1e5c4f3967740518897967125d41678","version_major":2,"version_minor":0},"text/plain":["PPO_LunarLander-v2.zip:   0%|          | 0.00/147k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["== CURRENT SYSTEM INFO ==\n","- OS: Linux-6.1.58+-x86_64-with-glibc2.35 # 1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\n","- Python: 3.10.12\n","- Stable-Baselines3: 2.0.0a5\n","- PyTorch: 2.2.1+cu121\n","- GPU Enabled: True\n","- Numpy: 1.25.2\n","- Cloudpickle: 2.2.1\n","- Gymnasium: 0.28.1\n","- OpenAI Gym: 0.25.2\n","\n","== SAVED MODEL SYSTEM INFO ==\n","- OS: Linux-6.1.58+-x86_64-with-glibc2.35 # 1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\n","- Python: 3.10.12\n","- Stable-Baselines3: 2.0.0a5\n","- PyTorch: 2.2.1+cu121\n","- GPU Enabled: True\n","- Numpy: 1.25.2\n","- Cloudpickle: 2.2.1\n","- Gymnasium: 0.28.1\n","- OpenAI Gym: 0.25.2\n","\n"]}],"source":["repo_id = 'GeorgeImmanuel/PPO_LunarLander-v2'\n","saved_file_name = 'PPO_LunarLander-v2.zip'\n","custom_objects = {\n","    'learning_rate':0.0,\n","    'lr_schedule':lambda _:0.0,\n","    'clip_range':lambda _:0.0\n","}\n","\n","checkpoint = load_from_hub(repo_id,saved_file_name)\n","model = PPO.load(checkpoint,custom_objects=custom_objects,print_system_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3273,"status":"ok","timestamp":1712680314252,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"WTLlix3prJ6W","outputId":"16d92e52-8d99-45c4-a394-2accdcc468e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Reward: 260.49052559999996 | Std Reward: 32.56641841025877\n"]}],"source":["# evaluate the loaded model\n","env_id = 'LunarLander-v2'\n","eval_env = Monitor(gym.make(env_id))\n","mean_reward, std_reward = evaluate_policy(model=model,env=eval_env,n_eval_episodes=10,deterministic=True)\n","\n","print('Mean Reward: {} | Std Reward: {}'.format(mean_reward,std_reward))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HYfnDmHrJ92"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Gjeye-1ZiWTH"},"source":["# openAI: introduction to RL\n","\n","* cumulative rewards are also known as returns\n","\n","## states and observations:\n","* states are complete information about the environment.\n","* observations are partial information about the environment.\n","\n","## Action-spaces:\n","* action space consists of set of all `valid` actions.\n","* discrete actions means finite number of actions.\n","* continous actions means infinite number of actions.\n","\n","## policy:\n","* policy is the rule the agent follows to take an action.\n","* policy is the agent's brain and you can interchangably use it for an agent for example it is apt to say something like: the policy is trying to find the ideal spot in the park, the policy is taking over the human labor in optimization tasks.\n","* polcies are deterministic and stochastic.\n","* deterministic policy means a specific policy is formulated for each state the agent can be in.\n","* stochastic policy means it gives probability distribution over the set of actions given the state of the environment.\n","\n","* two key computation that happen with stochastic policies are:\n","  1. sampling actions from the policy.\n","  2. computing the log likelihood of pariticular actions.$$ \\log \\pi_{\\theta}(a|s). $$\n","\n","## excerpt from the openAI website:\n","A categorical policy is like a classifier over discrete actions. You build the neural network for a categorical policy the same way you would for a classifier: the input is the observation, followed by some number of layers (possibly convolutional or densely-connected, depending on the kind of input), and then you have one final linear layer that gives you logits for each action, followed by a softmax to convert the logits into probabilities.\n","\n","Sampling. Given the probabilities for each action, frameworks like PyTorch and Tensorflow have built-in tools for sampling. For example, see the documentation for Categorical distributions in PyTorch, torch.multinomial, tf.distributions.Categorical, or tf.multinomial.\n","\n","Log-Likelihood. Denote the last layer of probabilities as $$ P_{\\theta}(s).$$ It is a vector with however many entries as there are actions, so we can treat the actions as indices for the vector. The log likelihood for an action a can then be obtained by indexing into the vector:\n","\n","$$ \\log \\pi_{\\theta}(a|s) = \\log \\left[P_{\\theta}(s)\\right]_a. $$\n","\n","## diagonal guassian policy:\n","A multivariate Gaussian distribution (or multivariate normal distribution, if you prefer) is described by a mean vector and a covariance matrix. A diagonal Gaussian distribution is a special case where the covariance matrix only has entries on the diagonal. As a result, we can represent it by a vector.\n","\n","A diagonal Gaussian policy always has a neural network that maps from observations to mean actions, $$ \\mu_{\\theta}(s)$$ . There are two different ways that the covariance matrix is typically represented.\n","\n","The first way: There is a single vector of log standard deviations, \\log \\sigma, which is not a function of state: the \\log \\sigma are standalone parameters. (You Should Know: our implementations of VPG, TRPO, and PPO do it this way.)\n","\n","The second way: There is a neural network that maps from states to log standard deviations, $$ \\log \\sigma_{\\theta}(s)$$ . It may optionally share some layers with the mean network.\n","\n","Note that in both cases we output log standard deviations instead of standard deviations directly. This is because log stds are free to take on any values in (-\\infty, \\infty), while stds must be nonnegative. It’s easier to train parameters if you don’t have to enforce those kinds of constraints. The standard deviations can be obtained immediately from the log standard deviations by exponentiating them, so we do not lose anything by representing them this way.\n","\n","Sampling. Given the mean action $$ \\mu_{\\theta}(s) $$ and standard deviation $$ \\sigma_{\\theta}(s), $$ and a vector z of noise from a spherical Gaussian $$ (z \\sim \\mathcal{N}(0, I)),$$ an action sample can be computed with\n","\n","$$ a = \\mu_{\\theta}(s) + \\sigma_{\\theta}(s) \\odot z $$\n","\n","where \\odot denotes the elementwise product of two vectors. Standard frameworks have built-in ways to generate the noise vectors, such as torch.normal or tf.random_normal. Alternatively, you can build distribution objects, eg through torch.distributions.Normal or tf.distributions.Normal, and use them to generate samples. (The advantage of the latter approach is that those objects can also calculate log-likelihoods for you.)\n","\n","Log-Likelihood. The log-likelihood of a k -dimensional action a, for a diagonal Gaussian with mean $$\\mu = \\mu_{\\theta}(s) $$ and standard deviation $$ \\sigma = \\sigma_{\\theta}(s)$$ , is given by\n","\n","$$ \\log \\pi_{\\theta}(a|s) = -\\frac{1}{2}\\left(\\sum_{i=1}^k \\left(\\frac{(a_i - \\mu_i)^2}{\\sigma_i^2} + 2 \\log \\sigma_i \\right) + k \\log 2\\pi \\right).$$\n","\n","\n","## trajectories:\n","* trajectories are sequence of states.\n","* for example $$ \\tau = \\left(s_0,s_1,s_2,s_3\\right) $$\n","* trajectories are also known as episodes or rollouts.\n","\n","## why a discount factor in reward equation?\n","\n","* finite-horizon discounted return: the time step is limited.\n","$$ R(\\tau) = r_t $$\n","* infinite-horizon discounted return: which is the sum of all rewards ever obtained by the agent, but discounted by how far off in the future they’re obtained. This formulation of reward includes a discount factor \\gamma \\in (0,1):\n","\n","$$ R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t. $$\n","\n","* Why would we ever want a discount factor, though? Don’t we just want to get all rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.\n","\n","## reward:\n","* Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes expected return when the agent acts according to it.\n","\n","* To talk about expected return, we first have to talk about probability distributions over trajectories.\n","\n","* Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a T -step trajectory is:\n","\n","* $$P(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t). $$\n","\n","The expected return (for whichever measure), denoted by J(\\pi), is then:\n","\n","$$ J(\\pi) = \\int_{\\tau} P(\\tau|\\pi) R(\\tau) = {\\tau\\sim \\pi}{R(\\tau)}. $$\n","\n","The central optimization problem in RL can then be expressed by\n","\n","$$ \\pi^* = \\arg \\max_{\\pi} J(\\pi) $$\n","\n","\n","## The Optimal Q-Function and the Optimal Action:\n","\n","There is an important connection between the optimal action-value function $$ Q^*(s,a) $$ and the action selected by the optimal policy. By definition, $$ Q^*(s,a)$$ gives the expected return for starting in state s, taking (arbitrary) action a, and then acting according to the optimal policy forever after.\n","\n","The optimal policy in s will select whichever action maximizes the expected return from starting in s. As a result, if we have Q^*, we can directly obtain the optimal action, $$ a^*(s)$$ , via\n","\n","$$ a^*(s) = \\arg \\max_a Q^* (s,a). $$\n","\n","Note: there may be multiple actions which maximize $$ Q^*(s,a), $$ in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action.\n","\n","## Bellman Equations\n","All four of the value functions obey special self-consistency equations called Bellman equations. The basic idea behind the Bellman equations is this:\n","\n","The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n","The Bellman equations for the on-policy value functions are\n","\n","$$ \\begin{align*}\n","V^{\\pi}(s) &= E{a \\sim \\pi \\\\ s'\\sim P}{r(s,a) + \\gamma V^{\\pi}(s')}, \\\\\n","Q^{\\pi}(s,a) &= E{s'\\sim P}{r(s,a) + \\gamma E{a'\\sim \\pi}{Q^{\\pi}(s',a')}},\n","\\end{align*} $$\n","\n","\n","\n","where s' \\sim P is shorthand for s' $$ \\sim P(\\cdot |s,a) $$, indicating that the next state s' is sampled from the environment’s transition rules; a \\sim \\pi is shorthand for a $$ \\sim \\pi(\\cdot|s);$$ and a' \\sim \\pi is shorthand for a' $$ \\sim \\pi(\\cdot|s').$$\n","\n","The Bellman equations for the optimal value functions are\n","\n","$$ \\begin{align*}\n","V^*(s) &= \\max_a E{s'\\sim P}{r(s,a) + \\gamma V^*(s')}, \\\\\n","Q^*(s,a) &= E{s'\\sim P}{r(s,a) + \\gamma \\max_{a'} Q^*(s',a')}.\n","\\end{align*} $$\n","\n","The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions, is the absence or presence of the \\max over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value.\n","\n","You Should Know\n","\n","The term “Bellman backup” comes up quite frequently in the RL literature. The Bellman backup for a state, or state-action pair, is the right-hand side of the Bellman equation: the reward-plus-next-value.\n","\n","\n","## Markov Decision Process:\n","Markov Decision Processes (MDPs). An MDP is a 5-tuple, $$ \\langle S, A, R, P, \\rho_0 \\rangle$$ , where\n","\n","* S is the set of all valid states,\n","* A is the set of all valid actions,\n","* R : S \\times A \\times S \\to \\mathbb{R} is the reward * function, with r_t = R(s_t, a_t, s_{t+1}),\n","* $$ P : S \\times A \\to \\mathcal{P}(S) $$ is the transition probability function, with $$ P(s'|s,a) $$ being the probability of transitioning into state s' if you start in state s and take action a,\n","and $$ \\rho_0 $$ is the starting state distribution.\n","The name Markov Decision Process refers to the fact that the system obeys the Markov property: transitions only depend on the most recent state and action, and no prior history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aH13hl7np64e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpQrHvByG0jD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SfdQ5ufG0mB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlpBrMPQG0pE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QY0um8y684sZ"},"source":["# openAI Gym:\n"]},{"cell_type":"markdown","metadata":{"id":"E4bC6v0cmjGc"},"source":["# day 351"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9692,"status":"ok","timestamp":1713542105550,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"FHDirPpYG1Uz","outputId":"43033b53-01f3-453b-f136-c8b2a9a86d8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["pip install -U gym"]},{"cell_type":"markdown","metadata":{"id":"nis_VAQLG1YT"},"source":["# our goal with openAI gym today:\n","\n","* drive a car up the hill.\n","* first let the car move back and forth to build up sufficient momentum to reach the top.\n","\n","![sfsfds](https://blog.paperspace.com/content/images/2020/11/mountain-car-v0.gif)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0c675fOBG1bC"},"outputs":[],"source":["import gym\n","env = gym.make('MountainCar-v0',render_mode='rgb_array')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713542106290,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"cjOy618-G1dr","outputId":"0e0c85c0-155d-40d6-fb95-c3958ad6517e"},"outputs":[{"output_type":"stream","name":"stdout","text":["observation space is: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n","action space is: Discrete(3)\n"]}],"source":["o_s = env.observation_space\n","a_s = env.action_space\n","\n","print('observation space is: {}'.format(o_s))\n","print('action space is: {}'.format(a_s))"]},{"cell_type":"markdown","metadata":{"id":"yp5PWqn1840b"},"source":["The observation for the mountain car environment is a vector of two numbers representing velocity and position. The middle point between the two mountains is taken to be the origin, with right being the positive direction and left being the negative direction.\n","\n","We see that both the observation space as well as the action space are represented by classes called Box and Discrete, respectively. These are one of the various data structures provided by gym in order to implement observation and action spaces for different kind of scenarios (discrete action space, continuous action space, etc).\n","\n","* Box(n,) corresponds to the n-dimensional continuous space. In our case n=2, thus the observational space of our environment is a 2-D space. Of course, the space is bounded by upper and lower limits which describe the legitimate values our observations can take. We can determine this using the high and low attributes of the observation space. These correspond to the maximum and minimum positions/velocities in our environment, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1713542109441,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"c5bzyNmIvOP3","outputId":"7a70e8c6-35c3-491c-d1fb-69f850f2fe2b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0.6 , 0.07], dtype=float32), array([-1.2 , -0.07], dtype=float32))"]},"metadata":{},"execution_count":4}],"source":["# maximum minimum positions/velocities in the environment respectively\n","o_s.high,o_s.low"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEH_SvH3vS-K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713542121922,"user_tz":-330,"elapsed":406,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"}},"outputId":"b28b912a-10f9-4f80-cde8-7f6b522ed699"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-0.5091937,  0.       ], dtype=float32), {})"]},"metadata":{},"execution_count":5}],"source":["env.reset()"]},{"cell_type":"markdown","metadata":{"id":"2Fgpd32gvwq1"},"source":["* The Discrete(n) box describes a discrete space with [0.....n-1] possible values. In our case n = 3, meaning our actions can take values of either 0, 1, or 2. Unlike Box, Discrete does not have a high and low method, since, by the very definition, it is clear what type of values are allowed.\n","\n","* If you try to input invalid values in the step function of our environment (in our case, say, 4), it will lead to an error."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1713542130036,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"UwSWwO-5v3Vk","outputId":"1ed764d3-3cb9-41c0-8e72-e946a20b1eae"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"execute_result","data":{"text/plain":["(array([-0.50830173,  0.000892  ], dtype=float32), -1.0, False, False, {})"]},"metadata":{},"execution_count":6}],"source":["env.step(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"elapsed":15,"status":"error","timestamp":1713542130718,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"Hq4RrtqMv3ZO","outputId":"203cd593-009c-4cef-d359-43b18f0d5137"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"4 (<class 'int'>) invalid","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-0597a87118df>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         assert self.action_space.contains(\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         ), f\"{action!r} ({type(action)}) invalid\"\n","\u001b[0;31mAssertionError\u001b[0m: 4 (<class 'int'>) invalid"]}],"source":["env.step(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFdZ4U0-wAKS"},"outputs":[],"source":["env.step(3)"]},{"cell_type":"code","source":["env.reset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGfnoFArEHx4","executionInfo":{"status":"ok","timestamp":1713542140562,"user_tz":-330,"elapsed":425,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"}},"outputId":"bb186bfe-54cc-4868-d265-deae0b31880b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-0.5449347,  0.       ], dtype=float32), {})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":548,"status":"ok","timestamp":1713542143406,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"9Y7i8Q2Iv3cH","outputId":"644f2543-f4ad-426f-820c-d3bc5e5afe42"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-0.54577476, -0.00084009], dtype=float32), -1.0, False, False, {})"]},"metadata":{},"execution_count":9}],"source":["env.step(0) # only 0,1,2 actions are allowed hence Discrete(3)"]},{"cell_type":"markdown","metadata":{"id":"cx4-3PkcwWes"},"source":["* There are multiple other spaces available for various use cases, such as MultiDiscrete, which allow you to use  more than one discrete variable for your observation and action space."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5RZqXyzrvwtt"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713542150115,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"Vp9aG9jjJZVB","outputId":"2d13d8af-2eeb-4495-9247-bfc196d74aa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["initial observation: (array([-0.59973645,  0.        ], dtype=float32), {})\n","new observation: [-0.59817034  0.00156608]\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","# reset the environment and see the initial observation\n","obs = env.reset()\n","print('initial observation: {}'.format(obs))\n","\n","# make the agent take a random action\n","random_action = env.action_space.sample()\n","\n","# step the agent up a notch\n","new_obs,reward,terminated,truncated,info = env.step(random_action)\n","\n","if terminated or truncated:\n","  env.reset()\n","\n","print('new observation: {}'.format(new_obs))\n","\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"K-sVLRFgJZXt"},"source":["* reset: This function resets the environment to its initial * state, and returns the observation of the environment corresponding to the initial state.\n","* step : This function takes an action as an input and applies it to the environment, which leads to the environment transitioning to a new state. The reset function returns four things:\n","* observation: The observation of the state of the environment.\n","* reward: The reward that you can get from the environment after executing the action that was given as the input to the step function.\n","terminated and truncated: Whether the episode has been terminated. If true, you may need to end the simulation or reset the environment to restart the episode.\n","* info: This provides additional information depending on the environment, such as number of lives left, or general information that may be conducive in debugging."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"elapsed":771,"status":"ok","timestamp":1713542150884,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"03OXCGF6N9z2","outputId":"52193362-9b56-4458-b3d0-030df0bb04ab"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# visualize the environment\n","img = env.render()\n","\n","plt.imshow(img)\n","plt.title(img.shape)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4OVJiUWfN95r"},"source":["If you want to see a screenshot of the game as an image, rather than as a pop-up window, you should set the mode argument of the render function to rgb_array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYY_u_uMRjVD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z49_BqKERjYL"},"outputs":[],"source":["# lets make the agent move a bit in the environment to figure out the optimal action\n","\n","import time\n","\n","obs = env.reset()\n","n_steps = 2500\n","\n","for step in range(n_steps):\n","  # you can also have the agent perform more intelligence by defigning its action as some intelligent function\n","  # like action = my_intelligent_fn(obs)\n","  # but for now we just let the agent do some random action in the beginning.\n","  action = env.action_space.sample()\n","\n","  # feed the action into the environment and get feedback from it.\n","  obs,reward,terminated,truncated,info = env.step(action)\n","\n","  # render the environment\n","  env.render()\n","\n","  # use some time delay for good visualization unless you want to see the car move at crazy speed.\n","  time.sleep(0.001)\n","\n","  if terminated or truncated:\n","    env.reset()\n","\n","# close the env\n","env.close()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYtfRxygRjbC"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gwWnSIRop676"},"source":["## Wrappers in OpenAI gym\n","\n","* The Wrapper class in OpenAI Gym provides you with the functionality to modify various parts of an environment to suit your needs. Why might such a need arise? Maybe you want to normalize your pixel input, or maybe you want to clip your rewards. While typically you could accomplish the same by making another class that sub-classes your environment Env class, the Wrapper class allows us to do it more systematically.\n","\n","* to appreciate the utility of wrappers we create a more comple atari game environment to train our mathematical agents in."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27486,"status":"ok","timestamp":1713542193671,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"},"user_tz":-330},"id":"KLgnuL7PUVGE","outputId":"43412163-0173-454a-bee2-abf1d8a25f14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.5.1)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.43.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n","Requirement already satisfied: ale-py~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.8.1)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (4.11.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --upgrade pip setuptools wheel\n","!pip install opencv-python\n","!pip install gym[atari]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1EbQRMy0h62","executionInfo":{"status":"ok","timestamp":1713542200288,"user_tz":-330,"elapsed":6623,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"}},"outputId":"c33a6bec-45b3-4d78-9db9-a93d7d5efde8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: atari-py==0.2.5 in /usr/local/lib/python3.10/dist-packages (0.2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==0.2.5) (1.25.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atari-py==0.2.5) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install atari-py==0.2.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15mcfrxe1VNw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713542202622,"user_tz":-330,"elapsed":2340,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"}},"outputId":"f81dda3a-9377-4298-cfb6-8d01ea247834"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: enum34 1.1.10\n","Uninstalling enum34-1.1.10:\n","  Successfully uninstalled enum34-1.1.10\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip uninstall -y enum34"]},{"cell_type":"code","source":["!pip install  enum34"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":239},"id":"PfoxMYhCDpkh","executionInfo":{"status":"ok","timestamp":1713542210976,"user_tz":-330,"elapsed":8357,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"}},"outputId":"7897cbe7-d71b-4fd4-b91b-e4ec7ceb5ad1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting enum34\n","  Using cached enum34-1.1.10-py3-none-any.whl.metadata (1.6 kB)\n","Using cached enum34-1.1.10-py3-none-any.whl (11 kB)\n","Installing collected packages: enum34\n","Successfully installed enum34-1.1.10\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["enum"]},"id":"ccf183c7e76c46e9858ea6b8d9473138"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idq4lPVdwpS-","executionInfo":{"status":"ok","timestamp":1713542229149,"user_tz":-330,"elapsed":4149,"user":{"displayName":"george emmanuel","userId":"18047942969920931469"}},"outputId":"a5a4bb7c-74f3-4815-87ee-e270fe856e04"},"outputs":[{"output_type":"stream","name":"stdout","text":["observation_space: Box(0, 255, (210, 160, 3), uint8)\n","action_space: Discrete(4)\n"]}],"source":["# lets create the environment\n","import time\n","import gym\n","env = gym.make(\"BreakoutNoFrameskip-v4\",render_mode='rgb_array')\n","\n","obs = env.reset()\n","\n","print(\"observation_space: {}\".format(env.observation_space))\n","print(\"action_space: {}\".format(env.action_space))\n","\n","n_steps = 1000\n","\n","for i in range(n_steps):\n","\n","  action = env.action_space.sample()\n","\n","  obs,reward,terminated,truncated,info = env.step(action)\n","\n","  env.render()\n","\n","  time.sleep(0.001)\n","\n","  if terminated or truncated:\n","    env.reset()\n","\n","env.close()\n"]},{"cell_type":"markdown","source":["* Our observation space is a continuous space of dimensions (210, 160, 3) corresponding to an RGB pixel observation of the same size. Our action space  contains 4 discrete actions (Left, Right, Do Nothing, Fire)"],"metadata":{"id":"E0dk_jBsDkUp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8RmjVYVwpWB"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["*  in Deep RL that we construct our observation by concatenating the past k frames together. We have to modify the Breakout Environment such that both our reset and step functions return concatenated observations.\n","* For this we define a class of type gym.Wrapper to override the reset and return functions of the Breakout Env. The Wrapper class, as the name suggests, is a wrapper on top of an Env class that modifies some of its attributes and functions.\n","* The __init__ function is defined with the Env class for which the wrapper is written, and the number of past frames to be concatenated. Note that we also need to redefine the observation space since we are now using concatenated frames as our observations. (We modify the observation space from (210, 160, 3) to (210, 160, 3 * num_past_frames.)"],"metadata":{"id":"U6OwLVGGwpZQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSbe1nwlUVIs"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"NNQ8hSEjvY1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LnDySbJrS3Ic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fh4cd3kyQeXQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOL/a8zX92uieXbSs+m8Cu7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00220cb4e429457895ec7653d1d2f613":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33aba1e32ad148aa88cad78cc766b07b","placeholder":"​","style":"IPY_MODEL_fbce52ebe9564935bc0ee6f9b5f87e6f","value":" 43.8k/43.8k [00:00&lt;00:00, 40.8kB/s]"}},"0200f0c5465c481da5c31aa9b788803e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d58646fce514d7a9793356c08f41a49","placeholder":"​","style":"IPY_MODEL_53d73ae308694e75b2920d908d874d16","value":" 864/864 [00:00&lt;00:00, 2.29kB/s]"}},"029afece5a8049f89bf4cd535ea78bf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a0f34697cf7449c9498f8b72860bef0","placeholder":"​","style":"IPY_MODEL_9ee2e49efa5b4a79a413809205d8790f","value":"policy.pth: 100%"}},"03e13db6a3c24f64b70f850dca92322d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77a50e308d9149de82cfef44e8fdafcc","max":88362,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62a8e31fcb4f4ebba9f2b86782f88250","value":88362}},"090327cc3d9c41829c87b4b02dd6c1f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a0f34697cf7449c9498f8b72860bef0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b1949c65ea642b4a79517850bbe8b48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"135538f49f6a488e8ad63e9210add5dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e5f9934f8f4ce3ae2e7a345c41cad3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"197de12f31784dc482f26a6a0af54c83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b1949c65ea642b4a79517850bbe8b48","placeholder":"​","style":"IPY_MODEL_5f305e4f4d134078a5da409732624d8b","value":" 147k/147k [00:00&lt;00:00, 48.5kB/s]"}},"1a0715148dc24fcd95aff806597c8bdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b2a589ce2204477aa957fd02f912f11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f01c451c1a3403caf3906d8ea018d47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96b81afe46c6462f90f42fd03fffeb9a","placeholder":"​","style":"IPY_MODEL_f65130317f904568af88e977c26c2f99","value":" 88.4k/88.4k [00:00&lt;00:00, 48.0kB/s]"}},"21cd05becd4d4b2faa7f7174c8dc97ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28663c1ad69f4403a4dfccbbd4f7d61b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33aba1e32ad148aa88cad78cc766b07b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a4d8d672cb745a2ab2a7b88617fecec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43e2055a262b458388f34f2993deb0aa","placeholder":"​","style":"IPY_MODEL_492fe7442f894b70b69d516f60827a93","value":"Your token has been saved to /root/.cache/huggingface/token"}},"3a87a7ea8f9e436b9994a9dbb5a83d78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c340e60a372495db0c6cc64563a3816":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4003ea162ddc44559e0c977f0adaecc1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee427dcae90141d0ae20a40503a3afd4","max":864,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13e5f9934f8f4ce3ae2e7a345c41cad3","value":864}},"409c4c28f0e44c1d93f99ae8e85e03a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"410f517dc8374ad49d7f6e82be12e42b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43e2055a262b458388f34f2993deb0aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48583169d0304dbc9f115d1f24509d78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492fe7442f894b70b69d516f60827a93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cbbbc7f0a1b4b59b731087915031863":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d58646fce514d7a9793356c08f41a49":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"518d48ea068e4494acead0ba36f9ee94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53d73ae308694e75b2920d908d874d16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54b904886b794392ba60cced4f3ab7b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54f0f6eb69c949e69ce834a6d170b767":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_029afece5a8049f89bf4cd535ea78bf8","IPY_MODEL_88a6b08a618e409c896d1579ea973056","IPY_MODEL_00220cb4e429457895ec7653d1d2f613"],"layout":"IPY_MODEL_3c340e60a372495db0c6cc64563a3816"}},"56ccab345a474dd2a5d08c005c553624":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e5e0523fb654afa9dbaea6d41b03452":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a87a7ea8f9e436b9994a9dbb5a83d78","placeholder":"​","style":"IPY_MODEL_56ccab345a474dd2a5d08c005c553624","value":" 4/4 [00:01&lt;00:00,  1.22s/it]"}},"5f305e4f4d134078a5da409732624d8b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62a8e31fcb4f4ebba9f2b86782f88250":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"630db6b4fbf248369882e1c816865899":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6518fca7c89a4dcab4d2a8cb4fe7c2f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93cad192bf93422bbbd84658d1894220","placeholder":"​","style":"IPY_MODEL_630db6b4fbf248369882e1c816865899","value":"Login successful"}},"70052eb56e604c1ab61d386d7169190d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0f04468fb3047b2ab5713d5735e312a","max":147369,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cbbbc7f0a1b4b59b731087915031863","value":147369}},"70313a3c9c4a4ea4bb393fb79acef9ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b726a2956bc421789eb0aaa6c089e35","IPY_MODEL_4003ea162ddc44559e0c977f0adaecc1","IPY_MODEL_0200f0c5465c481da5c31aa9b788803e"],"layout":"IPY_MODEL_21cd05becd4d4b2faa7f7174c8dc97ad"}},"71b3a711598c4b538f57b0c941d15734":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"753203c47baf46dd9b0a39bb48c8d093":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_135538f49f6a488e8ad63e9210add5dd","placeholder":"​","style":"IPY_MODEL_865fa67ba3f74344a7a1ed10fd18399d","value":"PPO_LunarLander-v2.zip: 100%"}},"77a50e308d9149de82cfef44e8fdafcc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"865fa67ba3f74344a7a1ed10fd18399d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88a6b08a618e409c896d1579ea973056":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c5733a85f174426870037bd4e314c54","max":43762,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71b3a711598c4b538f57b0c941d15734","value":43762}},"8c5733a85f174426870037bd4e314c54":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8df0ba72727c4799829504db9b3cae54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93cad192bf93422bbbd84658d1894220":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"952f0bb1e50f453a9edd0ee0e33b8e62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc87b4f7e3654f4ea4088a7dd678c3fa","IPY_MODEL_03e13db6a3c24f64b70f850dca92322d","IPY_MODEL_1f01c451c1a3403caf3906d8ea018d47"],"layout":"IPY_MODEL_1a0715148dc24fcd95aff806597c8bdd"}},"96b81afe46c6462f90f42fd03fffeb9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b726a2956bc421789eb0aaa6c089e35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c00af2b55f1b49768f168aa9c633ee16","placeholder":"​","style":"IPY_MODEL_409c4c28f0e44c1d93f99ae8e85e03a4","value":"pytorch_variables.pth: 100%"}},"9ee2e49efa5b4a79a413809205d8790f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f0a13718c0a4e1eae39f49e5153a5d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ca8500e1a342958730a630be636a46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9cbf25d0ccd43639fd50c98086d5c80","placeholder":"​","style":"IPY_MODEL_e538d432507743dcb0e8809a5a0a60f3","value":" 147k/147k [00:00&lt;00:00, 5.42MB/s]"}},"a39fb3c07ab74d409512a8205da77fbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48583169d0304dbc9f115d1f24509d78","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd70926e67654447bcd051a5b7e44188","value":4}},"a3bf406f9233420ea1da3ec4284bc652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c528df39478b43cbb5df852e73b8f29e","IPY_MODEL_70052eb56e604c1ab61d386d7169190d","IPY_MODEL_197de12f31784dc482f26a6a0af54c83"],"layout":"IPY_MODEL_c28aa1f610b443f1b8c24743a59bc41d"}},"a60fb77298244c2d98229e1478662317":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d5f8060cbd4e439b8205ac02779414":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"b10e2e10e80c4bf1a2cf7435f381ab2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1e5c4f3967740518897967125d41678":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_753203c47baf46dd9b0a39bb48c8d093","IPY_MODEL_b3de8947c1ef4960aefbb84a109d1c78","IPY_MODEL_a0ca8500e1a342958730a630be636a46"],"layout":"IPY_MODEL_9f0a13718c0a4e1eae39f49e5153a5d5"}},"b3de8947c1ef4960aefbb84a109d1c78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9838854dbab4f3cab812a45a784e73e","max":147369,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd0e4ff07b494b4594a0bd1495e63784","value":147369}},"b6189fe196944332bc70f2c694797626":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_090327cc3d9c41829c87b4b02dd6c1f3","placeholder":"​","style":"IPY_MODEL_54b904886b794392ba60cced4f3ab7b7","value":"Token is valid (permission: write)."}},"c00af2b55f1b49768f168aa9c633ee16":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c28aa1f610b443f1b8c24743a59bc41d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c528df39478b43cbb5df852e73b8f29e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28663c1ad69f4403a4dfccbbd4f7d61b","placeholder":"​","style":"IPY_MODEL_e9653923f56747b58ddf20f02ee891b9","value":"PPO_LunarLander-v2.zip: 100%"}},"d30d3e95b6854e55affc55c71f231962":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_b6189fe196944332bc70f2c694797626","IPY_MODEL_f31107e733044104ad975642193635c1","IPY_MODEL_3a4d8d672cb745a2ab2a7b88617fecec","IPY_MODEL_6518fca7c89a4dcab4d2a8cb4fe7c2f0"],"layout":"IPY_MODEL_a8d5f8060cbd4e439b8205ac02779414"}},"dd0e4ff07b494b4594a0bd1495e63784":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd70926e67654447bcd051a5b7e44188":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4fe14bb913e40b8a7447fe62a3ac963":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e538d432507743dcb0e8809a5a0a60f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9653923f56747b58ddf20f02ee891b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9838854dbab4f3cab812a45a784e73e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9cbf25d0ccd43639fd50c98086d5c80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6f7ec9dc914c3a9772935a643e0e1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1e34ebd19cc47d78b3916d597447393","IPY_MODEL_a39fb3c07ab74d409512a8205da77fbe","IPY_MODEL_5e5e0523fb654afa9dbaea6d41b03452"],"layout":"IPY_MODEL_410f517dc8374ad49d7f6e82be12e42b"}},"ee427dcae90141d0ae20a40503a3afd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0f04468fb3047b2ab5713d5735e312a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1e34ebd19cc47d78b3916d597447393":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a60fb77298244c2d98229e1478662317","placeholder":"​","style":"IPY_MODEL_1b2a589ce2204477aa957fd02f912f11","value":"Upload 4 LFS files: 100%"}},"f31107e733044104ad975642193635c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_518d48ea068e4494acead0ba36f9ee94","placeholder":"​","style":"IPY_MODEL_e4fe14bb913e40b8a7447fe62a3ac963","value":"Your token has been saved in your configured git credential helpers (store)."}},"f65130317f904568af88e977c26c2f99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbce52ebe9564935bc0ee6f9b5f87e6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc87b4f7e3654f4ea4088a7dd678c3fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b10e2e10e80c4bf1a2cf7435f381ab2a","placeholder":"​","style":"IPY_MODEL_8df0ba72727c4799829504db9b3cae54","value":"policy.optimizer.pth: 100%"}}}}},"nbformat":4,"nbformat_minor":0}