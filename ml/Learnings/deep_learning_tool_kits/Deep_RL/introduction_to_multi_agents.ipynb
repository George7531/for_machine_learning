{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZzBi1L5PHeoTjzDSWkwoB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/George7531/for_machine_learning/blob/main/ml/Learnings/deep_learning_tool_kits/Deep_RL/introduction_to_multi_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# day 403"
      ],
      "metadata": {
        "id": "0Zg8rgkoNzpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/thumbnail.png)\n",
        "Since the beginning of this course, we learned to train agents in a single-agent system where our agent was alone in its environment: it was not cooperating or collaborating with other agents.\n",
        "\n",
        "This worked great, and the single-agent system is useful for many applications.\n",
        "\n",
        "But, as humans, we live in a multi-agent world. Our intelligence comes from interaction with other agents. And so, our goal is to create agents that can interact with other humans and other agents.\n",
        "\n",
        "Consequently, we must study how to train deep reinforcement learning agents in a multi-agents system to build robust agents that can adapt, collaborate, or compete.\n",
        "\n",
        "So today we’re going to learn the basics of the fascinating topic of multi-agents reinforcement learning (MARL).\n",
        "\n",
        "And the most exciting part is that, during this unit, you’re going to train your first agents in a multi-agents system: a 2vs2 soccer team that needs to beat the opponent team.\n",
        "\n",
        "And you’re going to participate in AI vs. AI challenge where your trained agent will compete against other classmates’ agents every day and be ranked on a new leaderboard.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif)\n",
        "\n",
        "## takeaways:\n",
        "* we(every being living and non living) create greater things when colloborate with others."
      ],
      "metadata": {
        "id": "orpLZ4JDN6lV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tXn-EFARK2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-agents Reinforcement Learning:\n",
        "\n",
        "When we do multi-agents reinforcement learning (MARL), we are in a situation where we have multiple agents that share and interact in a common environment.\n",
        "\n",
        "For instance, you can think of a warehouse where multiple robots need to navigate to load and unload packages.\n",
        "\n",
        "\n",
        "![fsdfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/warehouse.jpg)\n",
        "\n",
        "Or a road with several autonomous vehicles.\n",
        "\n",
        "![fsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/selfdrivingcar.jpg)\n",
        "\n",
        "\n",
        "In these examples, we have multiple agents interacting in the environment and with the other agents. This implies defining a multi-agents system. But first, let’s understand the different types of multi-agent environments.\n",
        "\n",
        "\n",
        "## Different types of multi-agent environments\n",
        "Given that, in a multi-agent system, agents interact with other agents, we can have different types of environments:\n",
        "\n",
        "Cooperative environments: where your agents need to maximize the common benefits.\n",
        "For instance, in a warehouse, robots must collaborate to load and unload the packages efficiently (as fast as possible).\n",
        "\n",
        "Competitive/Adversarial environments: in this case, your agent wants to maximize its benefits by minimizing the opponent’s.\n",
        "For example, in a game of tennis, each agent wants to beat the other agent.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/tennis.png)\n",
        "\n",
        "Mixed of both adversarial and cooperative: like in our SoccerTwos environment, two agents are part of a team (blue or purple): they need to cooperate with each other and beat the opponent team.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif)\n",
        "\n",
        "So now we might wonder: how can we design these multi-agent systems? Said differently, how can we train agents in a multi-agent setting ?"
      ],
      "metadata": {
        "id": "hVfQPPMbRDLm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1G_mfB7gSoBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We have two solutions to design this multi-agent reinforcement learning system (MARL).\n",
        "\n",
        "## Decentralized systems\n",
        "![fsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/decentralized.png)\n",
        "\n",
        "In decentralized learning, each agent is trained independently from the others. In the example given, each vacuum learns to clean as many places as it can without caring about what other vacuums (agents) are doing.\n",
        "\n",
        "The benefit is that since no information is shared between agents, these vacuums can be designed and trained like we train single agents.\n",
        "\n",
        "The idea here is that our training agent will consider other agents as part of the environment dynamics. Not as agents.\n",
        "\n",
        "However, the big drawback of this technique is that it will make the environment non-stationary since the underlying Markov decision process changes over time as other agents are also interacting in the environment. And this is problematic for many Reinforcement Learning algorithms that can’t reach a global optimum with a non-stationary environment.\n",
        "\n",
        "## Centralized systems:\n",
        "![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/centralized.png)\n",
        "\n",
        "In this architecture, we have a high-level process that collects agents’ experiences: the experience buffer. And we’ll use these experiences to learn a common policy.\n",
        "\n",
        "For instance, in the vacuum cleaner example, the observation will be:\n",
        "\n",
        "The coverage map of the vacuums.\n",
        "The position of all the vacuums.\n",
        "We use that collective experience to train a policy that will move all three robots in the most beneficial way as a whole. So each robot is learning from their common experience. We now have a stationary environment since all the agents are treated as a larger entity, and they know the change of other agents’ policies (since it’s the same as theirs).\n",
        "\n",
        "If we recap:\n",
        "\n",
        "In a decentralized approach:\n",
        "  * we treat all agents independently without considering the existence of the other agents.\n",
        "  * In this case, all agents consider others agents as part of the environment.\n",
        "  * It’s a non-stationarity environment condition, so has no guarantee of convergence.\n",
        "\n",
        "In a centralized approach:\n",
        "  * A single policy is learned from all the agents.\n",
        "  * Takes as input the present state of an environment and * the policy outputs joint actions.\n",
        "The reward is global."
      ],
      "metadata": {
        "id": "v05gatzORBsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZu9mxB8gH2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep RL for Robotics, whether to drop unity course or continue it if you are really focused only on getting signals from the real world only?\n",
        "\n",
        "Robots receive streams of raw sensory observations as a consequence of their actions, and cannot practically obtain large amounts of detailed supervision beyond observing these sensor readings.\n",
        "This makes for a challenging but highly realistic learning\n",
        "problem. Further, unlike agents in video games, robots\n",
        "do not readily receive a score or reward function that is\n",
        "shaped for their needs, and instead need to develop their\n",
        "own internal representation of progress towards goals.\n",
        "\n",
        "## conclusion:\n",
        "* drop unity and learn gazebo for more precise simulation and also learn how to convert images from reality to simulation by taking a course on Generative Adversarial Network(GAN) so that the agent can see the reality just like the simulation hugely reducing the `reality gap`.\n",
        "\n",
        "## controller is the policy;\n",
        "We want to find a\n",
        "controller, (known as a policy in RL parlance), that maps\n",
        "states to actions in a way that maximizes the reward when\n",
        "executed.\n",
        "* controller,policy, decision algorithm are all same.\n",
        "\n",
        "## model based and model-free:\n",
        "model-based models: like using physics formula to compute the essential decision to make in a situation.\n",
        "model-fee models; learning by acting with the environment and then forming a model.\n",
        "## problems with grasping objects:\n",
        "* knowing which object to grasp.\n",
        "* knowing the location on the object to grasp.\n",
        "\n",
        "## how deep RL helps with grasping?\n",
        "* The lessons from this work have been that\n",
        "* (1) a lot of\n",
        "varied data was required to learn generalizable grasping,\n",
        "which means that we need unattended data collection and\n",
        "a scalable RL pipeline;\n",
        "* (2) the need for large and varied data\n",
        "means that we need to leverage all of the previously collected\n",
        "data so far (offline data) and need a framework that makes\n",
        "this easy is crucial;\n",
        "* (3) to achieve maximal performance,\n",
        "combining offline data with a small amount of online data\n",
        "allows us to go from 86% to 96% grasp success.\n",
        "\n",
        "\n",
        "## Need for simulation:\n",
        "* cost effective.\n",
        "* no/less real life hardware damages or less damages if any.\n",
        "* faster training with increased compute.\n",
        "* data collection from virtual environment for the agent is free compared to manual data collection to train real robots without simulation.\n",
        "* we have to build simulation that are close to reality in everything to deploy the techniques learned in simulation to the real world.\n",
        "\n",
        "### Addressing Partial Observations In simulation:\n",
        "we can access the ground-truth state of the robot, which can\n",
        "significantly simplify the learning of tasks. In contrast, in the\n",
        "real-world, we are restricted to partial observations that are\n",
        "usually noisy and delayed, due to the limitation of onboard\n",
        "sensors. For example, it is difficult to precisely measure\n",
        "the root translation of a legged robot. To eliminate this\n",
        "difference, we can remove the inaccessible states during\n",
        "training (Tan et al. 2018), apply state estimation, add more\n",
        "sensors (e.g. Motion Capture) (Haarnoja et al. 2019) or learn\n",
        "to infer the missing information (e.g. reward) (Yang et al.\n",
        "2019). On the other hand, if used properly, the groundtruth states in simulation can significantly speed up learning.\n",
        "“Learning by cheating” (Chen et al. 2020) first leveraged the\n",
        "ground-truth states to learn a privileged agent, and in the\n",
        "second stage, imitated this agent to remove the reliance on\n",
        "the privileged information\n",
        "\n",
        "### Better Simulation: `The reality gap is caused by\n",
        "the discrepancy between the simulation and the real-world\n",
        "physics`. This error has many sources, including incorrect\n",
        "physical parameters, un-modeled dynamics, and stochastic\n",
        "real environment. However, there is no general consensus\n",
        "about which of these sources plays a more important role.\n",
        "After a large number of experiments with legged robots, both\n",
        "in simulation and on real robots, we found that the actuator\n",
        "dynamics and the lack of latency modeling are the main\n",
        "causes of the model error. `Developing accurate models for\n",
        "the actuator and latency significantly narrow the reality gap`\n",
        "(Tan et al. 2018). We successfully deployed agile locomotion\n",
        "gaits that are learned in simulation to the real robot without\n",
        "the need for any data collected on the robot.\n",
        "\n",
        "### Domain Randomization\n",
        "The idea behind domain randomization is to randomly sample different simulation\n",
        "parameters while training the RL policy. This can include\n",
        "various dynamics parameters (Peng et al. 2018b; Tan\n",
        "et al. 2018) of the robot and the environment, as well\n",
        "as visual and rendering parameters such as textures and\n",
        "lighting (Sadeghi and Levine 2017; Tobin et al. 2017).\n",
        "Similar to data augmentation methods in supervised learning,\n",
        "policies trained under such diverse conditions tend to be\n",
        "more robust to such variations, and can thus perform better\n",
        "in the real-world\n",
        "\n",
        "### Domain adaptation:\n",
        "Domain Adaptation The success of adversarial\n",
        "training methods such as generative adversarial networks\n",
        "(GAN) (Goodfellow et al. 2014) have resulted in their\n",
        "application to several other problems, including sim-to-real\n",
        "transfer. Adapter networks have been trained that convert\n",
        "simulated images to look like their real-world counterparts,\n",
        "which can then be used to train policies in simulation (James\n",
        "et al. 2017; Shrivastava et al. 2017; Bousmalis et al. 2017,\n",
        "2018; Rao et al. 2020). An alternative approach is that\n",
        "of James et al. (2019) which trains an adapter network to\n",
        "convert real-world images to canonical simulation images,\n",
        "allowing a policy trained only in simulation to be applied\n",
        "in the real-world. Training of the real-to-sim adapter was\n",
        "achieved by using domain-randomized simulation images as\n",
        "a proxy for real-world images, removing the need for realworld data altogether. The resulting policy achieved 70%\n",
        "grasp success in the real-world with the QT-Opt algorithm,\n",
        "with no real-world data, and reaches a success rate of 91%\n",
        "after fine-tuning on just 5,000 real-world grasps: a result\n",
        "which previously took over 500,000 grasps to ach\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xWVmxq0LQ1yO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzV4eoJcjYJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# self play: a classic techinique to train competitive Agents in adversarial Games.\n",
        "\n",
        "![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif)\n",
        "\n",
        "## what is SelfPlay:\n",
        "\n",
        "Training agents correctly in an adversarial game can be quite complex.\n",
        "\n",
        "On the one hand, we need to find how to get a well-trained opponent to play against your training agent. And on the other hand, if you find a very good trained opponent, how will your agent improve its policy when the opponent is too strong?\n",
        "\n",
        "Think of a child that just started to learn soccer. Playing against a very good soccer player will be useless since it will be too hard to win or at least get the ball from time to time. So the child will continuously lose without having time to learn a good policy.\n",
        "\n",
        "The best solution would be to have an opponent that is on the same level as the agent and will upgrade its level as the agent upgrades its own. Because if the opponent is too strong, we’ll learn nothing; if it is too weak, we’ll overlearn useless behavior against a stronger opponent then.\n",
        "\n",
        "This solution is called self-play. In self-play, the agent uses former copies of itself (of its policy) as an opponent. This way, the agent will play against an agent of the same level (challenging but not too much), have opportunities to gradually improve its policy, and then update its opponent as it becomes better. It’s a way to bootstrap an opponent and progressively increase the opponent’s complexity.\n",
        "\n",
        "It’s the same way humans learn in competition:\n",
        "\n",
        "We start to train against an opponent of similar level\n",
        "Then we learn from it, and when we acquire some skills, we can move further with stronger opponents.\n",
        "We do the same with self-play:\n",
        "\n",
        "We start with a copy of our agent as an opponent this way, this opponent is on a similar level.\n",
        "We learn from it and, when we acquire some skills, we update our opponent with a more recent copy of our training policy.\n",
        "The theory behind self-play is not something new. It was already used by Arthur Samuel’s checker player system in the fifties and by Gerald Tesauro’s TD-Gammon in 1995. If you want to learn more about the history of self-play[A blog post by Andrew Cohen](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents)\n",
        "\n",
        "## Self play in unity ml agents:\n",
        "Self-Play is integrated into the MLAgents library and is managed by multiple hyperparameters that we’re going to study. But the main focus, as explained in the documentation, is the tradeoff between the skill level and generality of the final policy and the stability of learning.\n",
        "\n",
        "Training against a set of slowly changing or unchanging adversaries with low diversity results in more stable training. But a risk to overfit if the change is too slow.\n",
        "\n",
        "So we need to control:\n",
        "\n",
        "How often we change opponents with the swap_steps and team_change parameters.\n",
        "The number of opponents saved with the window parameter. A larger value of window  means that an agent’s pool of opponents will contain a larger diversity of behaviors since it will contain policies from earlier in the training run.\n",
        "The probability of playing against the current self vs opponent sampled from the pool with play_against_latest_model_ratio. A larger value of play_against_latest_model_ratio  indicates that an agent will be playing against the current opponent more often.\n",
        "The number of training steps before saving a new opponent with save_steps parameters. A larger value of save_steps  will yield a set of opponents that cover a wider range of skill levels and possibly play styles since the policy receives more training.\n",
        "To get more details about these hyperparameters, you definitely need to check out [this part of the documentation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#self-play)\n",
        "\n",
        "## The ELO Score to evaluate our agent\n",
        "What is ELO Score?\n",
        "In adversarial games, tracking the cumulative reward is not always a meaningful metric to track the learning progress: because this metric is dependent only on the skill of the opponent.\n",
        "\n",
        "Instead, we’re using an ELO rating system (named after Arpad Elo) that calculates the relative skill level between 2 players from a given population in a zero-sum game.\n",
        "\n",
        "In a zero-sum game: one agent wins, and the other agent loses. It’s a mathematical representation of a situation in which each participant’s gain or loss of utility is exactly balanced by the gain or loss of the utility of the other participants. We talk about zero-sum games because the sum of utility is equal to zero.\n",
        "\n",
        "This ELO (starting at a specific score: frequently 1200) can decrease initially but should increase progressively during the training.\n",
        "\n",
        "The Elo system is inferred from the losses and draws against other players. It means that player ratings depend on the ratings of their opponents and the results scored against them.\n",
        "\n",
        "Elo defines an Elo score that is the relative skills of a player in a zero-sum game. We say relative because it depends on the performance of opponents.\n",
        "\n",
        "The central idea is to think of the performance of a player as a random variable that is normally distributed.\n",
        "\n",
        "The difference in rating between 2 players serves as the predictor of the outcomes of a match. If the player wins, but the probability of winning is high, it will only win a few points from its opponent since it means that it is much stronger than it.\n",
        "\n",
        "After every game:\n",
        "\n",
        "The winning player takes points from the losing one.\n",
        "The number of points is determined by the difference in the 2 players ratings (hence relative).\n",
        "If the higher-rated player wins → few points will be taken from the lower-rated player.\n",
        "If the lower-rated player wins → a lot of points will be taken from the high-rated player.\n",
        "If it’s a draw → the lower-rated player gains a few points from the higher.\n",
        "So if A and B have rating Ra, and Rb, then the expected scores are given by:\n",
        "\n",
        "![fsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/elo1.png)\n",
        "\n",
        "Then, at the end of the game, we need to update the player’s actual Elo score. We use a linear adjustment proportional to the amount by which the player over-performed or under-performed.\n",
        "\n",
        "We also define a maximum adjustment rating per game: K-factor.\n",
        "\n",
        "K=16 for master.\n",
        "K=32 for weaker players.\n",
        "If Player A has Ea points but scored Sa points, then the player’s rating is updated using the formula:\n",
        "\n",
        "![fsdsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/elo2.png)\n",
        "\n",
        "## Refer this page for example problems:\n",
        "[kundi 🎭](https://huggingface.co/learn/deep-rl-course/unit7/self-play)"
      ],
      "metadata": {
        "id": "rikTsd3n9Ok1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import sympy as sym\n",
        "\n",
        "Rb = 1253\n",
        "Ra = 2832\n",
        "\n",
        "Eb = 1/(1+10**((Ra-Rb)/400))\n",
        "Ea = 1/(1+10**((Rb-Ra)/400))\n",
        "Eb,Ea # expected probability of winning and losing."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXrQzcqk3O5W",
        "outputId": "1d95687b-cfff-450f-de5d-5c60f7198ecc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.00011283686015894695, 0.999887163139841)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if the players are renowned skillful players then they would get k_master else k_normal\n",
        "k_master = 32\n",
        "k_normal = 16\n",
        "\n",
        "# sa = 1 if won or sa = 0 if lost."
      ],
      "metadata": {
        "id": "ZG8vVYY53r2C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# player b wins\n",
        "sb = 1\n",
        "delta_rb = Rb + k_normal*(sb-Eb)\n",
        "\n",
        "delta_rb # his score would change from 1253 to ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bpu4bYz2lgT",
        "outputId": "3b19c5ce-8a71-40bd-a7c7-408b469dbf17"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1268.9981946102375"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# player b loses\n",
        "sb = 1\n",
        "delta_rb = Rb + k_normal*(sb-Eb)\n",
        "\n",
        "delta_rb # his score would change from 1253 to ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6xfrytr5tYK",
        "outputId": "a5e9f7ff-f54d-4d82-eee7-c5a487c1298a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1268.9981946102375"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# player a losses\n",
        "sa = 0\n",
        "delta_ra = Ra + k_normal*(sa-Ea)\n",
        "\n",
        "delta_ra # his score would change from 2832 to ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ54rifm2Xht",
        "outputId": "e6a77fc9-e5bc-44e3-945c-302f46640a11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2816.0018053897625"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# player a wins\n",
        "sa = 1\n",
        "delta_ra = Ra + k_normal*(sa-Ea)\n",
        "\n",
        "delta_ra # his score would change from 2832 to ? That's minute, freaking change."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4E9e_Ju11aG",
        "outputId": "4ffe85e0-f2ba-4e1b-df55-a4bbf0ce6913"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2832.0018053897625"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oY8Qxo-q0sTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hands-on will be different since to get correct results you need to train your agents from 4 hours to 8 hours. And given the risk of timeout in Colab, we advise you to train on your computer. You don’t need a supercomputer: a simple laptop is good enough for this exercise."
      ],
      "metadata": {
        "id": "Ti41fHBqPuWH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AhZvrXjd6gr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVI3Ls-X6gul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yq2kUmKPuY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyDbko-vPubm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}