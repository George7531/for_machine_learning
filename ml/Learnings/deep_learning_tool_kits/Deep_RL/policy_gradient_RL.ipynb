{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmI6CHiIRVR/ZK/+9fSVTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/George7531/for_machine_learning/blob/main/ml/Learnings/deep_learning_tool_kits/Deep_RL/policy_gradient_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# day 375"
      ],
      "metadata": {
        "id": "aJF638Jm37QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png)\n",
        "\n",
        "* In deep q-learing, a value-based deep reinforcement learning algorithm, we used a deep neural network to approximate the different Q-values for each possible action at a state.\n",
        "\n",
        "\n",
        "![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg)\n",
        "\n",
        "* In value-based methods, the policy (œÄ) only exists because of the action value estimates since the policy is just a function (for instance, greedy-policy) that will select the action with the highest value given a state.\n",
        "\n",
        "* With policy-based methods, we want to optimize the policy directly without having an intermediate step of learning a value function.\n",
        "\n",
        "* So now, we‚Äôll learn about policy-based methods and study a subset of these methods called policy gradient. Then we‚Äôll implement our first policy gradient algorithm called Monte Carlo Reinforcement algorithm from scratch using PyTorch. Then, we‚Äôll test its robustness using the CartPole-v1 and PixelCopter environments.\n",
        "\n",
        "## what are policy based methods:\n",
        "* The main goal of Reinforcement learning is to find the optimal policy $ùúã^‚àó$ that will maximize the expected cumulative reward. Because Reinforcement Learning is based on the reward hypothesis: all goals can be described as the maximization of the expected cumulative reward.\n",
        "\n",
        "* For instance, in a soccer game (where you‚Äôre going to train the agents in two units), the goal is to win the game. We can describe this goal in reinforcement learning as maximizing the number of goals scored (when the ball crosses the goal line) into your opponent‚Äôs soccer goals. And minimizing the number of goals in your soccer goals.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg)\n",
        "\n",
        "# value-based, policy-based and actor critic method:\n",
        "* In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy $ ùúã^‚àó $.\n",
        "\n",
        "* In value-based methods, we learn a value function.The idea is that an optimal value function leads to an optimal policy\n",
        "$ ùúã^‚àó $ .\n",
        "* Our objective is to minimize the loss between the predicted and target value to approximate the true action-value function.\n",
        "* We have a policy, but it's implicit since it is generated directly from the value function. For instance, in Q-Learning, we used an (epsilon-)greedy policy.\n",
        "\n",
        "* On the other hand, in policy-based methods, we directly learn to approximate\n",
        "$ ùúã^‚àó $ without having to learn a value function.\n",
        "\n",
        "* The idea is to parameterize the policy. For instance, using a neural network $ ùúã_{\\theta} $, this policy will output a probability distribution over actions (stochastic policy).\n",
        "\n",
        "![sff](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png)\n",
        "\n",
        "* Our objective then is to maximize the performance of the parameterized policy using gradient ascent.\n",
        "* To do that, we control the parameter ùúÉ that will affect the distribution of actions over a state.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png)\n",
        "* Next time, we'll study the actor-critic method, which is a combination of value-based and policy-based methods.\n",
        "\n",
        "\n",
        "## Difference between Policy based method and Policy gradient method:\n",
        "\n",
        "* Policy-gradient methods, what we‚Äôre going to study in this unit, is a subclass of policy-based methods. In policy-based methods, the optimization is most of the time on-policy since for each update, we only use data (trajectories) collected by our most recent version of $ ùúã_{ùúÉ} $\n",
        "\n",
        "\n",
        "* The difference between these two methods lies on how we optimize the parameter:\n",
        "\n",
        "  1. In policy-based methods, we search directly for the optimal policy. We can optimize the parameter $ \\theta $ indirectly by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.\n",
        "  2. In policy-gradient methods, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter ùúÉ directly by performing the gradient ascent on the performance of the objective function.\n",
        "  \n",
        "\n",
        "## Advantages and Disadvantages of policy gradient methods:\n",
        "* Before diving more into how policy-gradient methods work (the objective function, policy gradient theorem, gradient ascent, etc.), let‚Äôs study the advantages and disadvantages of policy-based methods.\n",
        "\n",
        "### Advantages:\n",
        "1. The simplicity of integration: we can directly estimate the policy without storing the additional data(Action values).\n",
        "\n",
        "2. Policy-gradient methods can learn a stochastic policy\n",
        "Policy-gradient methods can learn a stochastic policy while value functions can‚Äôt.\n",
        "\n",
        "  This has two consequences:\n",
        "\n",
        "    1. we don‚Äôt need to implement an exploration/exploitation trade-off by hand. Since we output a probability distribution over actions, the agent explores the state space without always taking the same trajectory.\n",
        "\n",
        "    2. We also get rid of the problem of perceptual aliasing. Perceptual aliasing is when two states seem (or are) the same but need different actions.\n",
        "\n",
        "Let‚Äôs take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoid killing the hamsters.\n",
        "\n",
        "![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg)\n",
        "\n",
        "\n",
        "* Our vacuum cleaner can only perceive where the walls are.\n",
        "\n",
        "* The problem is that the two red (colored) states are aliased states(in one you have to move left to and another you have to right to), because the agent perceives an upper and lower wall for each.\n",
        "\n",
        "![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg)\n",
        "\n",
        "* Under a deterministic policy, the policy will either always move right when in a red state or always move left. Either case will cause our agent to get stuck and never suck the dust. perhaps this may have caused our shooting space agent to move from enemies gunfire in an attempt to evade their attack to maximize the game score but never did it once shoot back at the enemies.\n",
        "\n",
        "* Under a value-based Reinforcement learning algorithm, we learn a quasi-deterministic policy (‚Äúgreedy epsilon strategy‚Äù). Consequently, our agent can spend a lot of time before finding the dust.\n",
        "\n",
        "* On the other hand, an optimal stochastic policy will randomly move left or right in red (colored) states. Consequently, it will not be stuck and will reach the goal state with a high probability.\n",
        "\n",
        "![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg)\n",
        "\n",
        "\n",
        "3. Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces:\n",
        "\n",
        "  * The problem with Deep Q-learning is that their predictions assign a score (maximum expected future reward) for each possible action, at each time step, given the current state.\n",
        "\n",
        "  * But what if we have an infinite possibility of actions?\n",
        "\n",
        "  * for instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15¬∞, 17.2¬∞, 19,4¬∞, honking, etc.). We‚Äôll need to output a Q-value for each possible action! And taking the max action of a continuous output is an optimization problem itself!\n",
        "\n",
        "  * Instead, with policy-gradient methods, we output a probability distribution over actions.\n",
        "\n",
        "\n",
        "4. Q value function aggressively chooses values that will not be very good in a continous action space where subtelity is required:\n",
        "\n",
        "  * Policy-gradient methods have better convergence properties\n",
        "  In value-based methods, we use an aggressive operator to change the value function: we take the maximum over Q-estimates. Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.\n",
        "\n",
        "  * For instance, if during the training, the best action was left (with a Q-value of 0.22) and the training step after it‚Äôs right (since the right Q-value becomes 0.23), we dramatically changed the policy since now the policy will take most of the time right instead of left.\n",
        "\n",
        "  * on the other hand, in policy-gradient methods, stochastic policy action preferences (probability of taking action) change smoothly over time.\n",
        "\n",
        "\n",
        "### Disadvantages:\n",
        " Naturally, policy-gradient methods also have some disadvantages:\n",
        "\n",
        "    * Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.\n",
        "    * Policy-gradient goes slower, step by step: it can take longer to train (inefficient).\n",
        "    * Policy-gradient can have high variance. We‚Äôll see in the actor-critic unit why, and how we can solve this problem.\n"
      ],
      "metadata": {
        "id": "2d5fmMbm4MTy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_ZdC3xXFLqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diving Deeper into policy gradient method:\n",
        "\n",
        "\n",
        "## The big picture:\n",
        "* We just learned that policy-gradient methods aim to find parameters ùúÉ that maximize the expected return.\n",
        "\n",
        "* The idea is that we have a parameterized stochastic policy. In our case, a neural network outputs a probability distribution over actions. The probability of taking each action is also called the action preference.\n",
        "\n",
        "If we take the example of CartPole-v1:\n",
        "\n",
        "    * As input, we have a state.\n",
        "    * As output, we have a probability distribution over actions at that state.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png)\n",
        "\n",
        "* Our goal with policy-gradient is to control the probability distribution of actions by tuning the policy such that good actions (that maximize the return) are sampled more frequently in the future. Each time the agent interacts with the environment, we tweak the parameters such that good actions will be sampled more likely in the future.\n",
        "\n",
        "### But how are we going to optimize the weights using the expected return?\n",
        "\n",
        "* The idea is that we‚Äôre going to let the agent interact during an episode. And if we win the episode, we consider that each action taken was good and must be more sampled in the future since they lead to win.\n",
        "\n",
        "* So for each state-action pair, we want to increase the\n",
        "P(a‚à£s): the probability of taking that action at that state. Or decrease if we lost.\n",
        "\n",
        "* The Policy-gradient algorithm (simplified) looks like this:\n",
        "![fsfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_bigpicture.jpg)\n",
        "\n",
        "* Now that we got the big picture, let‚Äôs dive deeper into policy-gradient methods.\n",
        "\n",
        "\n",
        "## Diving Deeper into Policy Gradient Methods:\n",
        "* We have our stochastic policy ùúã which has a parameter ùúÉ.This œÄ, given a state, outputs a probability distribution of actions.\n",
        "\n",
        "![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png)\n",
        "\n",
        "* But how do we know if our policy is good? We need to have a way to measure it. To know that, we define a score/objective function called $ J(\\theta)$ .\n",
        "\n",
        "\n",
        "### The Objective Function:\n",
        "* The objective function gives us the performance of the agent given a trajectory (state action sequence without considering reward (contrary to an episode)), and it outputs the expected cumulative reward.\n",
        "\n",
        "![sfsddf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/objective.jpg)\n",
        "\n",
        "* let's give some detail on this formula:\n",
        "* The expected return (also called expected cumulative reward), is the weighted average (where the weights are given by P(œÑ:Œò) of all possible values that return R(œÑ) can take.\n",
        "\n",
        "![sfsd](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/expected_reward.png)\n",
        "\n",
        "* R(œÑ) : Return from an arbitrary trajectory. To take this quantity and use it to calculate the expected return, we need to multiply it by the probability of each possible trajectory.\n",
        "\n",
        "* P(œÑ;Œ∏) : Probability of each possible trajectory œÑ (that probability depends on Œ∏ since it defines the policy that it uses to select the actions of the trajectory which has an impact of the states visited).\n",
        "\n",
        "![ssfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/probability.png)\n",
        "\n",
        "* J(Œ∏) : Expected return, we calculate it by summing for all trajectories, the probability of taking that trajectory given Œ∏ multiplied by the return of this trajectory.\n",
        "\n",
        "* Our objective then is to maximize the expected cumulative reward by finding the Œ∏ that will output the best action probability distributions\n",
        "\n",
        "* Œ∏ will output the best action probability distributions.\n",
        "\n",
        "![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/max_objective.png)\n",
        "\n",
        "\n",
        "### Gradient Ascent and Policy Gradient Theorem:\n",
        "\n",
        "[To understand Gradient Descent and Gradient Ascent check this out!!!](https://www.baeldung.com/cs/gradient-descent-vs-ascent)\n",
        "\n",
        "\n",
        "* Policy-gradient is an optimization problem: we want to find the values of Œ∏ that maximize our objective function\n",
        "J(Œ∏), so we need to use gradient-ascent. It‚Äôs the inverse of gradient-descent since it gives the direction of the steepest increase of J(Œ∏).\n",
        "* Our update step for gradient-ascent is:\n",
        "$ Œ∏‚ÜêŒ∏+Œ±‚àó‚àá_{\\theta}J(Œ∏)$ We can repeatedly apply this update in the hopes that Œ∏ converges to the value that maximizes J(Œ∏).\n",
        "\n",
        "* However, there are two problems with computing the derivative of J(Œ∏):\n",
        "\n",
        "* We can‚Äôt calculate the true gradient of the objective function since it requires calculating the probability of each possible trajectory, which is computationally super expensive. So we want to calculate a gradient estimation with a sample-based estimate (collect some trajectories).\n",
        "\n",
        "* We have another problem that I explain in the next optional section. To differentiate this objective function, we need to differentiate the state distribution, called the `Markov  Decision Process dynamics`. This is attached to the environment. It gives us the probability of the environment going into the next state, given the current state and the action taken by the agent. The problem is that we can‚Äôt differentiate it because we might not know about it.\n",
        "\n",
        "![sfsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/probability.png)\n",
        "\n",
        "* Fortunately we‚Äôre going to use a solution called the Policy Gradient Theorem that will help us to reformulate the objective function into a differentiable function that does not involve the differentiation of the state distribution.\n",
        "\n",
        "* If you want to understand how we derive this formula for approximating the gradient, keep on reading. I will discuss it on the next cell.\n",
        "\n",
        "## The Reinforce algorithm(Monte Carlo Reinforce):\n",
        "* The Reinforce algorithm, also called Monte-Carlo policy-gradient, is a policy-gradient algorithm that uses an estimated return from an entire episode(Trajectory(tau)) to update the policy parameter Œ∏:\n",
        "\n",
        "* In a loop:\n",
        "\n",
        "    1. Use the policy $œÄ_Œ∏$ to collect an episode œÑ\n",
        "\n",
        "    2. Use the episode to estimate the gradient $\\hat{g}=‚àá_Œ∏ J(Œ∏)$\n",
        "\n",
        "    ![sfs](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_one.png)\n",
        "\n",
        "    3. Update the weights of the policy: $Œ∏‚ÜêŒ∏+Œ±*\\hat{g}$\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "* $‚àá_{\\theta}logœÄ_{\\theta}(a_t|s_t)R(\\tau) \\Rightarrow $ is the direction of steepest increase of the (log) probability of selecting action $a_t$ from state $s_t$ . This tells us how we should change the weights of policy if we want to increase/decrease the log probability of selecting action $a_t$ from the state $s_t$\n",
        "\n",
        "* R(œÑ): is the scoring function:\n",
        "    1. If the return is high, it will push up the probabilities of the (state, action) combinations.\n",
        "    2. Otherwise, if the return is low, it will push down the probabilities of the (state, action) combinations.\n",
        "\n",
        "\n",
        "### Multiple Trajectories:\n",
        "* We can also collect multiple episodes (trajectories) to estimate the gradient:\n",
        "![fsf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_multiple.png)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1j9yqU8wFIeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVqrC6W_kLIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradient Theorem(Extra deep explanation):\n",
        "\n",
        "* we‚Äôre going to study how we differentiate the objective function that we will use to approximate the policy gradient.\n",
        "\n",
        "1. The objective function:\n",
        "![sfsdf](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/expected_reward.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NdMPA9FBjpUb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILMYInOnrAr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_XBm2otsjjlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aC2NRfbi1HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkaBTagf4MWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}